%% Fitting Damon's Model to ORN Data
% In this document, I will try to fit the Dynamical Adaptaiton (DA) model to data. The Dynamical Adaptaiton model has 8 parameters that need to be constrained. Setting one of them, $\tau_{r}$, to zero eliminates the need to solve a differential equation, and the response is given in closed form as a funciton of the stimulus. This greatly speeds up code execution. We also constrain $n_{y}$ and $n_{z}$ to default values of 2. 

%% Fitting DA Model to synthetic data 
% Fitting these 7 parameters to data is a hard problem. First, I will try to fit these 7 parameters to synthetic data, generated by the DA model. We generate some synthetic stimulus using random noise inputs filtered by some low pass function (see top panel in plot below). 

% plotting params
marker_size = 16;
marker_size2 = 32;
font_size = 24;
cfilter_length = 333;

% pick some random parameters
p.A = 100; % alpha in model
p.B = 1; % beta in the model
p.C = 0.2; % gamma in the model
p.tau_y = 10;
p.n_y = 2;
p.tau_z = 60;
p.n_z = 2;
p.tau_r = 0; % we set tau_r to zero to start with


% random noise stimulus
stimulus = 1+(2*randn(1,11000));
stimulus = filter(ones(1,100)/100,1,stimulus);
stimulus(1:1000)= [];
R = DA_integrate(stimulus,p);

% show the stimulus and the output
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
mpo.LineWidth = 2;
mpo.Color = 'k';
mpo.font_size = 24;
multiplot([],stimulus,R,mpo), hold on

% now run the GA estimation ~50 times to get a sense of how well we can extract the parameters of the model
load ensemblep.mat
nrep = 50 - length(ensemblep);
for i = 1:nrep
	% make new random stimuli
	stimulus = 1+(2*randn(1,11000));
	stimulus = filter(ones(1,100)/100,1,stimulus);
	stimulus(1:1000)= [];
	R = DA_integrate(stimulus,p);

	% and run GA
	FitnessFunction = @(x) DA_cost_function(x,stimulus,R,@Cost);
	gaoptions = gaoptimset('Display','iter','UseParallel',true,'Generations',500,'TolFun',1e-6);
	lb = [0 0 0 0 2 0 2];
	ub = [mean(R)*10 10 1 1e2 3 1e3 3];
	[x,fval] = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
	ensemblep = [ensemblep ValidateDAParameters(x)];
	save('ensemblep.mat','ensemblep');
end
clear i


% now find the result from the guess
Rguess = DA_integrate(stimulus,ensemblep(end));
plot(Rguess,'r','LineWidth',2)
% now find the linear prediction
filter_length = 333;
K = FindBestFilter(stimulus,R);
fp = filter(K,1,stimulus-mean(stimulus)) + mean(R); 
plot(fp,'g','LineWidth',2)
legend({'Response','DA Prediction','Linear Prediction'});

%%
% Then, choosing some arbitrary parameters for the DA model, we generate the DA model output. This is shown in the black line in the figure below (the bottom panel). Using this model output and the known input, we run a global optimisation procedure (a genetic algorithm) on this data set fifty times, and estimate the parameters that give predicted responses with lowest least square error. One such solution is shown below in red, together with a simple linear prediction from a linear filter in green.

%%
% OK, so the genetic algorithm seems to find some parameters that do an OK job of predicting the response. How good is the estimate of the parameters of the model itself? In the following figure I plot the ensemble predictions from the 50 different realisations of the optimisation algorithm, normalised by the value of each parameter to visualse how well we can estimate the "real" parameters of the model. 
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
for i = 1:7
	x =  i*ones(1,50)+0.1*rand(1,50);
	switch i
	case 1
		y=[ensemblep.A]./p.A;
		text(i,min(y)/2,'\alpha')
	case 2
		y=[ensemblep.B]./p.B;
		text(i,min(y)/2,'\beta')
	case 3
		y=[ensemblep.C]./p.C;
		text(i,min(y)/2,'\gamma')
	case 4
		y=[ensemblep.tau_y]./p.tau_y;
		text(i,min(y)/2,'\tau_{y}')
	case 5
		y=[ensemblep.n_y]./p.n_y;
		text(i,min(y)/2,'n_{y}')
	case 6
		y=[ensemblep.tau_z]./p.tau_z;
		text(i,min(y)/2,'\tau_{z}')
	case 7
		y=[ensemblep.n_z]./p.n_z;
		text(i,min(y)/2,'n_{z}')
	end
	scatter(x,y,64)

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 7.5],'XTickLabel',{})

%%
% That's quite a spread. To understand whether this spread is because our optimisation algorithm is fundamentally flawed, or to see if the mapping from the space of model parameters to model output is many to one, we plot the r-square of predicted response vs. the absolute error of the prediction of the model parameters. In this plot, each color is a different parameter, and the colors correspond to colors in previous plots. 

%%
% Ignoring for the moment the fact that there are two clusters, and concentrating only on the better-fit cluster (the one on the right), we see no correlation between quality of fit (as measured by r-square) and the relative absolute difference between estimated parameter and actual parameter of the DA model.

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1000 700]); hold on
for i = 1:7
	x =  zeros(1,50);
	for j = 1:50
		Rguess = DA_integrate(stimulus,ensemblep(j));
		x(j) = rsquare(R,Rguess);
	end
	clear j
	switch i
	case 1
		y=abs([ensemblep.A]-p.A)./p.A;
	case 2
		y=abs([ensemblep.B]-p.B)./p.B;
	case 3
		y=abs([ensemblep.C]-p.C)./p.C;
	case 4
		y=abs([ensemblep.tau_y]-p.tau_y)./p.tau_y;
	case 5
		y=abs([ensemblep.n_y]-p.n_y)./p.n_y;
	case 6
		y=abs([ensemblep.tau_z]-p.tau_z)./p.tau_z;
	case 7
		y=abs([ensemblep.n_z]-p.n_z)./p.n_z;
	end
	scatter(x,y,64,'filled')

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 1])
xlabel('rsquare of prediction and data','FontSize',font_size)
ylabel('Relative absolute difference b/w params.','FontSize',font_size)



%% Fitting DA Model to real data
% Now we understand how the fitting works, we will try to fit real data. The following figure shows the sample data we use. The top panel shows the stimulus as measured by the PID, and the bottom panel shows the real firing rate of the ORN. Both the PID and the ORN firing rate have been rescaled so that they range from [0,1]. 

if ~exist('PID','var')
	filename = '/data/random-stim/final_2011_06_14_ab3A_1o3ol3X-3_20ml_30sec_30ms_rand.mat';
	[PID, time, f,Valve] = PrepData3(filename);
end

% make all vectors consistent
PID = PID(:)-min(PID);
PID = PID*100;
Valve= Valve(:) - min(Valve);
Valve = Valve/max(Valve);
time = time(:);
f = f(:) - min(f);
f = f/100;


figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
multiplot(time,PID,f);
set(gca,'XLim',[20 30])
PrettyFig;

%%
% No we try to fit the DA model to this data, as before. 

if ~exist('pidp','var')
	FitnessFunction = @(x) DA_cost_function(x,PID,f,@Cost);
	gaoptions = gaoptimset('Display','iter','UseParallel',true,'Generations',500,'TolFun',1e-6);
	lb = [0 0 0 0 2 0 2];
	ub = [1e4 1000 1 1e6 10 1e6 10];
	[x,fval] = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
	pidp = ValidateDAParameters(x);


end

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
fpDA = DA_integrate(PID,pidp);
multiplot(time,f,fpDA);
set(gca,'XLim',[20 30])
PrettyFig;

%%
% This is horrible. I don't know why the fit doesn't work. Instead, I'm going to try fitting to the valve signal and see if that works. 


if ~exist('vp','var')
	FitnessFunction = @(x) DA_cost_function(x,Valve,f,@Cost);
	gaoptions = gaoptimset('Display','none','UseParallel',true,'Generations',500,'TolFun',1e-6);
	lb = [0 0 0 0 2 0 2];
	ub = [1e4 1000 1 1e6 2 1e6 2];
	[x,fval] = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
	vp = ValidateDAParameters(x);
end

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
fpDA = DA_integrate(Valve,vp);
multiplot(time,f,fpDA);
set(gca,'XLim',[20 30])
PrettyFig;

return
% close all since we're publishing this document
close all

%% Real Data: Gain Analysis

history_lengths = [102];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,fp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('Actual Model output (a.u.)','FontSize',font_size)



