%% Fitting the Dynamical Adaptation Model to ORN Responses
% In this document, I will try to fit the Dynamical Adaptation (DA) model to data from random stimulation experiments on ORNs. First, the DA model is fit to synthetic data generated by the DA model, and then is fit to real data. I then go through the analysis of the instantaneous gain for a linear model and the DA model, and characterize the fast adaptation properties of ORNs to this odour. 

%  ######  ##    ## ##    ## ######## ##     ## 
% ##    ##  ##  ##  ###   ##    ##    ##     ## 
% ##         ####   ####  ##    ##    ##     ## 
%  ######     ##    ## ## ##    ##    ######### 
%       ##    ##    ##  ####    ##    ##     ## 
% ##    ##    ##    ##   ###    ##    ##     ## 
%  ######     ##    ##    ##    ##    ##     ## 




% ########     ###    ########    ###           
% ##     ##   ## ##      ##      ## ##          
% ##     ##  ##   ##     ##     ##   ##         
% ##     ## ##     ##    ##    ##     ##        
% ##     ## #########    ##    #########        
% ##     ## ##     ##    ##    ##     ##        
% ########  ##     ##    ##    ##     ##        


%% Synthetic Data: Fitting the DA model
% The Dynamical Adaptation model has 8 parameters that need to be constrained. Setting one of them, $\tau_{r}$, to zero eliminates the need to solve a differential equation, and the response is given in closed form as a function of the stimulus. This greatly speeds up code execution.  

% plotting params
marker_size = 6;
marker_size2 = 32;
font_size = 24;
cfilter_length = 333;

% pick some random parameters
p.A = 100; % alpha in model
p.B = 35; % beta in the model
p.C = 0.3; % gamma in the model
p.tau_y = 1;
p.n_y = 2;
p.tau_z = 10;
p.n_z = 2;
p.tau_r = 0; % we set tau_r to zero to start with



% now run the GA estimation 
if ~exist('sdatap.mat','file')
	% make the stimulus
	stimulus = exp(randn(1,11000));
	%stimulus = abs(filter(ones(1,10)/10,1,stimulus));
	R = DA_integrate(stimulus,p);
	% normalise R
	R = R/std(R);
	R = R - mean(R);
	R = R + 0.2*randn(1,length(R));

	stimulus(1:1000) = [];
	R(1:1000) = [];

	% run pattern search
	data.PID = stimulus;
	data.f = R;
	x0 = [100 35 0.3 1 2 10 2];
	lb = [10 0 0 0 2 0 2];
	ub = [500 50 1 10 10 100 10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	sdatap = ValidateDAParameters(x,'ga');

	save('sdatap.mat','sdatap','stimulus','R')
end
load sdatap.mat

% show the stimulus and the output
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
mpo.LineWidth = 2;
mpo.Color = 'k';
mpo.font_size = 24;
ah=multiplot([],stimulus,R,mpo); hold on



% now find the result from the guess
Rguess = DA_integrate(stimulus,sdatap);
Rguess = Rguess/std(Rguess);
Rguess = Rguess - mean(Rguess);
plot(Rguess,'r','LineWidth',2)
% now find the linear prediction
filter_length = 333;
K = FindBestFilter(stimulus,R);
sdatafp = filter(K,1,stimulus-mean(stimulus)); 
plot(sdatafp,'g','LineWidth',2)
legend({'Response','DA Prediction','Linear Prediction'});
set(gca,'XLim',[2000 2500])
title(ah(1),'Figure 1: Fitting DA model to synthetic data')
PrettyFig;

% censor data
R = R(100:end);
Rguess = Rguess(100:end);
sdatafp=sdatafp(100:end);
stimulus = stimulus(100:end);


%%
% First, I will fit these 7 parameters to synthetic data, generated by running the DA model on exponentiated and filtered gaussian random noise input. In Figure 1, the top panel shows the input to the model. The bottom panel shows the model output (black), the linear prediction (green), and finally the DA model prediction (red).

%%
% The DA model fits the data really well; the black line of the synthetic data is completely hidden by the red line of the DA model prediction. To fit the DA model, I used a genetic algorithm to perform a nearly-global search with integer constraints on $n_{y}$ and $n_{z}$, and $\gamma$ constrained to $[0,1]$

%%
% The DA model seems to do a pretty good job estimating the data output. Is it better than the simple linear prediction? Here, we compare the r-square and the l-2 norm between the data and the fit. For the simple linear model, the r-square is 
disp(rsquare(R(1000:end),sdatafp(1000:end)))

%%
% cf. the DA model fit, the rsquare is 
disp(rsquare(R(1000:end),Rguess(1000:end)))

%% 
% The l-2 norm of the linear fit is
disp(l2(R(1000:end),sdatafp(1000:end)))

%% 
% cf. l-2 norm of the DA fit is
disp(l2(R(1000:end),Rguess(1000:end)))

%%
% The DA model fits the data much better, which makes sense as the stimulus is non-gaussian, and we have crafted the synthetic data set so that the DA model does the best job explaining its variance. 

%% Synthetic Data: Gain Analysis
% Sensors can exhibit fast adaptation to the stimulus, on a time-scale not dissimilar to the time-scale of response to the stimulus. In this analysis, we smooth the stimulus to the sensor over some arbitrary history window, and plot the actual response of the sensor to the model prediction for the top 10% of smoothed stimulus input, and for the bottom 10% of the smoothed stimulus input. 

%%
% In the figure below, we characterise the "fast adaptation" properties in the synthetic data.  The plot on the left compares the data (on the Y-axis) to the linear fit, while the plot on the right compares the data to the DA model fit. Several interesting features are visible: 


%% 
% # The best-fit lines to the top 10% (red) and bottom 10% (red) of stimulus have different slopes in the linear fit. In particular, the response to the stronger stimuli (red) has lower gain (smaller slope) than the response to the lower stimuli (green). 
% # This is not true for the fit to the DA model. The DA model does not systematically wrongly estimate the gain of the synthetic data, unlike the linear fit. 


history_lengths = [30];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,sdatafp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('Synthetic Data (a.u.)','FontSize',font_size)


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R,Rguess,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})


%%
% How does this vary with the length of the history window?  In the analysis above, we have kept the "window history" length fixed. How does varying this window change the separation of slopes of best fit lines for the top 10% and the bottom 10%? 

history_lengths = [30 102 150 300 600 1002 1500];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 450],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,sdatafp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Linear Prediction','FontSize',font_size)
set(gca,'YLim',[0.6 1.7])

% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R,Rguess,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.6 1.7])

%  #######  ########   #######  ##     ## ########        
% ##     ## ##     ## ##     ## ##     ## ##     ##       
% ##     ## ##     ## ##     ## ##     ## ##     ##       
% ##     ## ##     ## ##     ## ##     ## ########        
% ##     ## ##     ## ##     ## ##     ## ##   ##         
% ##     ## ##     ## ##     ## ##     ## ##    ##        
%  #######  ########   #######   #######  ##     ##       


% ########  ##     ## ##        ######  ########  ######  
% ##     ## ##     ## ##       ##    ## ##       ##    ## 
% ##     ## ##     ## ##       ##       ##       ##       
% ########  ##     ## ##        ######  ######    ######  
% ##        ##     ## ##             ## ##             ## 
% ##        ##     ## ##       ##    ## ##       ##    ## 
% ##         #######  ########  ######  ########  ######  
                                                      
                                                                                          
                                                                                                                         

%% Real Data 1: Responses to increasing concentration of odor 
% The simplest stimulus to give to a ORN is a pulse of odor, whose amplitude we can vary. These experiments are routinely done in measuring the dose-response properties of ORNs. The following data is from fig 3 of Carlotta's paper, and shows the firing rate histograms of neuron ab3A in response to increasing pulses of ethyl acetate. 

%%
% In the following plot, the top row shows, from left to right, the stimulus presented to the ORN, the response of the ORN, and predictions from the linear model, the NLN model, and the DA model. 

% grab data
data=load('/local-data/orn/carlotta-paper/fig3abc.mat');
dt = data.time(2)-data.time(1);
mean_stimulus = mean(data.stimulus(120:140,:));
peak_response = max(data.response);

% make figure
figure('outerposition',[0 0 1500 850],'PaperUnits','points','PaperSize',[1500 850]); hold on
for i = 1:5
	a(i)=subplot(2,5,i);
	hold on
end
clear i
a(6) = subplot(2,2,3); hold on; % dose response
set(a(6),'XScale','log');
a(7) = subplot(2,2,4); hold on; % response time vs amplitude
set(a(7),'XScale','log');

% plot PID
plot(a(1),data.time,data.stimulus)
set(a(1),'XLim',[0.5 2])
ylabel(a(1),'Stimulus amplitude (a.u.)')

% plot ORN
plot(a(2),data.time,data.response)
set(a(2),'XLim',[0.5 2])
ylabel(a(2),'ORN Response (Hz)')
plot(a(6),mean_stimulus,peak_response,'x-k')
data.rt  = dt*FindPeakResponseTimes(data.response,96);
plot(a(7),mean_stimulus,data.rt,'x-k')
title(a(2),'Data')
ylim = get(a(2),'YLim');
set(a(2),'YLim',ylim);


% plot linear fit 
OnlyThesePoints = zeros*data.stimulus;
OnlyThesePoints(50:end,:) = 1;
OnlyThesePoints = find(OnlyThesePoints(:));
[K diagnostics] = FindBestFilter(data.stimulus(:),data.response(:),OnlyThesePoints,'filter_length=50;');
data.LinearFit = filter(K,1,data.stimulus(:)-mean(data.stimulus(:))) + mean(data.response(:));
data.LinearFit = reshape(data.LinearFit,length(data.time),length(data.rt));
peak_response_linear = max(data.LinearFit);
plot(a(3),data.time,data.LinearFit)
set(a(3),'XLim',[0.5 2],'YLim',ylim)
title(a(3),'Linear Fit','Color','b')
plot(a(6),mean_stimulus,peak_response_linear,'x-b')
data.rt_linear  = dt*FindPeakResponseTimes(data.LinearFit,96);
plot(a(7),mean_stimulus,data.rt_linear,'x-b')

% fit NLN Model
data2.response = data.response(:);
data2.stimulus = data.stimulus(:);
if ~exist('NLNFit','var')
	[NLNFit, x] = FitNLNModel(data2);
end

% plot NLN model
% LNFit = FitNonLinearity(data.LinearFit(:),data.response(:));
NLNFit = reshape(NLNFit,length(data.time),length(data.rt));
plot(a(4),data.time,NLNFit)
set(a(4),'XLim',[0.5 2],'YLim',ylim)
title(a(4),'NLN Model','Color','g')
peak_response_LN = max(NLNFit);
data.rt_LN  = dt*FindPeakResponseTimes(NLNFit,96);
plot(a(6),mean_stimulus,peak_response_LN,'x-g')
plot(a(7),mean_stimulus,data.rt_LN,'x-g')

% plot DA model
if isfield(data,'DAFit')
	% no need to optimise again
	plot(a(5),data.time,data.DAFit)
	set(a(5),'XLim',[0.5 2],'YLim',ylim)
	peak_response_DA = max(data.DAFit);
	plot(a(6),mean_stimulus,peak_response_DA,'x-r')
	data.rt_DA  = dt*FindPeakResponseTimes(data.DAFit,96);
	plot(a(7),mean_stimulus,data.rt_DA,'x-r')
	title(a(5),'DA Fit','Color','r')

else
	% fit DA model to data
	[p, DAFit] = FitDAModelToData(data);
	% save
	save('/local-data/orn/carlotta-paper/fig3abc.mat','DAFit','p','-append');

end


% some cosmetic changes
ylabel(a(6),'Peak Response (Hz)')
xlabel(a(6),'Mean Stimulus Amplitude (V)')
ylabel(a(7),'Peak Response Time (s)')
xlabel(a(7),'Mean Stimulus Amplitude (V)')
PrettyFig;


%%
% What is the DA model actually doing? 
% [~,y,z] = DA_integrate2(data.stimulus(:),data.p);
% g = 1./(1+data.p.B+z);
% y = reshape(y,1090,10);
% z = reshape(z,1090,10);
% g = reshape(g,1090,10);


% my = max(y);
% mz = max(z);
% for i = 1:10
% 	y(:,i) = y(:,i)/my(i);
% 	z(:,i) = z(:,i)/mz(i);
% end

% y(:,1:3) = []; z(:,1:3) = [];

% figure('outerposition',[0 0 1000 800],'PaperUnits','points','PaperSize',[1000 800]); hold on
% plot(data.time,y,'-.','LineWidth',2)
% plot(data.time,z,'LineWidth',2)
% set(gca,'XLim',[0 3],'YLim',[-0.01 1.1],'LineWidth',2,'FontSize',20)


% ##      ## ######## ########  ######## ########  ####  ######  
% ##  ##  ## ##       ##     ## ##       ##     ## #### ##    ## 
% ##  ##  ## ##       ##     ## ##       ##     ##  ##  ##       
% ##  ##  ## ######   ########  ######   ########  ##    ######  
% ##  ##  ## ##       ##     ## ##       ##   ##              ## 
% ##  ##  ## ##       ##     ## ##       ##    ##       ##    ## 
%  ###  ###  ######## ########  ######## ##     ##       ######  


% ##          ###    ##      ##                                  
% ##         ## ##   ##  ##  ##                                  
% ##        ##   ##  ##  ##  ##                                  
% ##       ##     ## ##  ##  ##                                  
% ##       ######### ##  ##  ##                                  
% ##       ##     ## ##  ##  ##                                  
% ######## ##     ##  ###  ###                                   


%% ORN response to pulses on top of a background 

%%
% first we do a simulation using the DA model fitted to the dose-responses to see if the model predicts if the Weber-Fechner law is followed.
p = data.p; % use parameters from dose-response fit

% make stimuli
dt = 0.003; % 3ms, same as dose-response data
t = dt:dt:19;
background_conc =  [0 0.001 0.01 0.1 0.25 0.5 1 1.5 2 2.5 3 5 7 9];
stim = zeros(length(t),length(background_conc));
for i = 1:length(background_conc)
	% add background 
	stim(floor(3/dt):floor(16/dt),i) = background_conc(i);

	% add foreground
	stim(floor(13/dt):floor(13.5/dt),i) = background_conc(i)+ 10;
end

% calculate DA model predictions 
resp = NaN*stim;
for i = 1:length(background_conc)
	resp(:,i) = DA_integrate2(stim(:,i),p);
end

% calculate sensitivity
max_resp = max(resp(4000:5000,:));
%max_resp  = max_resp - mean(resp(4000:4200,:));
sensitivity = max_resp/max_resp(1);
plot(max(stim(1:1000,:)),sensitivity,'r.-')
set(gca,'XScale','log')


% ########     ###    ##    ## ########  
% ##     ##   ## ##   ###   ## ##     ## 
% ##     ##  ##   ##  ####  ## ##     ## 
% ########  ##     ## ## ## ## ##     ## 
% ##   ##   ######### ##  #### ##     ## 
% ##    ##  ##     ## ##   ### ##     ## 
% ##     ## ##     ## ##    ## ########  



%  ######  ######## #### ##     ##       
% ##    ##    ##     ##  ###   ###       
% ##          ##     ##  #### ####       
%  ######     ##     ##  ## ### ##       
%       ##    ##     ##  ##     ##       
% ##    ##    ##     ##  ##     ##       
%  ######     ##    #### ##     ##       


%% ORN responses to flickering olfactory stimulus. 
% Now that we know that we can fit the model, and that our optimisation algorithm works, we will try to fit real data. The following figure shows the sample data we use. The top panel shows the stimulus as measured by the PID, and the bottom panel shows the firing rate of the ORN. The firing rate has been divided by its standard deviation and has been mean subtracted. Also shown is the linear prediction of the firing rates. 


if ~exist('PID','var')
	filename = '~/Desktop/final_2011_06_14_ab3A_1o3ol3X-3_20ml_30sec_30ms_rand.mat';
	[PID, time, f,Valve,uncropped] = PrepData3(filename);
	PID = PID(:);
	time = time(:);
	f = f(:);

	% detrend PID
	ptrend = fit(time,PID,'Poly1'); 
	PID = PID - (ptrend(time) - mean(ptrend(time)));

	% prepare data
	f = f/std(f);
	f = f(:) - mean(f);


	% assemble into a data structure
	data.PID = PID;
	data.f = f;
	data.Valve = Valve;
end


% build a simple linear model
K = FindBestFilter(PID(500:end),f(500:end),[],'min_cutoff=min(response);');
LinearFit = filter(K,1,PID-mean(PID))+ mean(f(500:end));
LinearFit(LinearFit<min(f)) = min(f);

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
a=multiplot(time,PID,f,LinearFit);
title(a(1),'Figure 2: ORN data, and linear model prediction')
set(gca,'XLim',[20 25])
PrettyFig;


%%
% Now, we fit the DA model to the data using a pattern search optimisation (the type of optimisation doesn't matter. Pattern search is faster and converges to local minima faster than GA). $\gamma$ is constrained to $[0,1]$

% use pattern search to find parameters
if ~exist('psp.mat','file')
	x0 = [3557  203   0.35 1.8 1.9   17   1];
	lb = [1200  30    0    0   1     0    1];
	ub = [5200  300   1    10  5     30   5];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	psp = ValidateDAParameters(x,'ga');
	save('psp.mat','psp')
end

load psp.mat
figure('outerposition',[0 0 1400 500],'PaperUnits','points','PaperSize',[1400 500]); hold on
DAFit = DA_integrate(PID,psp);
DAFit = DAFit - mean(DAFit(500:end));
DAFit(DAFit<min(f))=min(f);

mpo = [];
mpo.legend = 1;

subplot(1,2,1), hold on
multiplot(time,f,LinearFit,DAFit,mpo);
set(gca,'XLim',[min(time)+1 min(time)+3])
PrettyFig;

mpo.legend = 0;

subplot(1,2,2),  hold on
multiplot(time,f,LinearFit,DAFit,mpo);
set(gca,'XLim',[mean(time)-1 mean(time)+1])
set(gca,'YTick',[])
set(gca,'YColor','w')
PrettyFig;






%%
% The DA model seems to do a pretty good job estimating the ORN output. Is it better than the simple linear prediction? Here, we compare the r-square and the l-2 norm between the data and the fit. For the simple linear model, the r-square is 
s = 500;
z = length(f);
disp(rsquare(f(s:end),LinearFit(s:end)))

%%
% cf. for the DA model fit, the rsquare is 
disp(rsquare(f(s:end),DAFit(s:end)))

%% 
% The l-2 norm of the linear fit is
disp(l2(f(s:end),LinearFit(s:end)))

%% 
% cf. l-2 norm of the DA fit is
disp(l2(f(s:end),DAFit(s:end)))



%% Real Data: Gain Analysis
% We can perform a similar gain analysis like we did on the synthetic data on the real data from the ORN. The plot on the left compares the ORN response data (on the Y-axis) to the linear fit, while the plot on the right compares the data to the DA model fit. 
s = 300; % when we start for the gain analysis
z = length(f); % where we end
history_lengths = [0.102];
% hl = history_lengths/3; % history lengths better be divisible by 3!
% shat = NaN(length(hl),length(PID(s:z)));
% for i = 1:length(hl)
% 	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,PID(s:z));
% 	shat(i,1:hl(i)) = NaN;
% end

x.data = f(s:z);
x.prediction = LinearFit(s:z);
x.stimulus = PID(s:z);
x.time = time(s:z);

% make gain analysis plot for synthetic data and linear model
figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
plothere=subplot(1,2,1); hold on
GainAnalysis2(x,history_lengths,filter_length,'plotid',1,'plothere',plothere);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('ORN response (a.u.)','FontSize',font_size)

% make gain analysis plot for synthetic data and DA model
x.data = f(s:z);
x.prediction = DAFit(s:z);
x.stimulus = PID(s:z);
x.time = time(s:z);
plot_here=subplot(1,2,2); hold on
GainAnalysis2(x,history_lengths,filter_length,'plotid',1,'plothere',plothere);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})



%%
% In the analysis above, we have kept the "window history" length fixed at ~100ms. How does varying this window change the separation of slopes of best fit lines for the top 10% and the bottom 10%? 

%%
% And we can do the same thing for the ORN response data.

s = 300; % when we start for the gain analysis
history_lengths = [.030 .102 .150 .300 0.600 1.002 1.500 2.001];

x.data = f(s:z);
x.prediction = LinearFit(s:z);
x.stimulus = PID(s:z);
x.time = time(s:z);


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plothere=subplot(1,2,1); hold on
GainAnalysis2(x,history_lengths,filter_length,'plotid',2,'plothere',plothere);
title('Linear Prediction','FontSize',font_size)
set(gca,'YLim',[0.7 1.5])

x.prediction = DAFit(s:z);
% make gain analysis plot for synthetic data and DA model
plothere=subplot(1,2,2); hold on
GainAnalysis2(x,history_lengths,filter_length,'plotid',2,'plothere',plothere);
ylabel('ORN response (a.u.)','FontSize',font_size)
legend('Location',[0.7674    0.5927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.7 1.5])




return






























% Why are the slopes not the same? One possibility is that the stimulus isn't actually the stimulus that the fly sees. We can check for this effect by filtering the PID, and creating fake neuron outputs using the DA model, and then trying to fit a DA model to the filtered stimulus. 


% First, we filter the stimulus to mimic hypothesised low-pass filtering of the actual stimulus by the PID. 
fPID = filtfilt(ones(1,50)/50,1,PID);
figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
multiplot(time,PID,fPID)
set(gca,'XLim',[15 20],'FontSize',font_size,'LineWidth',2)


% We then generate a synthetic output from the unfiltered stimulus using the DA model, and add some noise to it, and fit the DA model to it to check that the function minimisation works with this stimulus. 
load('fakeORNp.mat')
SynData = DA_integrate(PID,fakeORNp);
SynData(1:100) = mean(SynData(100:end));
SynData = SynData - mean(SynData);
SynData = SynData + 0.2*randn(length(SynData),1);

% now fit a DA model to this to check that everything works well
if ~exist('p2.mat','file')
	data.PID = PID;
	data.f = SynData;
	x0 = [3245 34 0.3 2  2   16 1];
	lb = [3000 20 0   0  2   0  1 ];
	ub = [5000 50 1   10 10 100 10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	p2 = ValidateDAParameters(x,'ga');
	save('p2.mat','p2')

end

load p2.mat
PredictionFromStim = DA_integrate(PID,p2);
PredictionFromStim = PredictionFromStim - mean(PredictionFromStim(100:end));

figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
multiplot(time,SynData,PredictionFromStim);
set(gca,'XLim',[15 20],'FontSize',font_size,'LineWidth',2)


% When we use the stimulus and the synthetic data from the DA model, we can recover the gain equalisation as before:

s = 300; % when we start for the gain analysis
z = length(SynData); % where we end
history_lengths = [102];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(PID(s:z)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,PID(s:z));
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 600 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
plot_here = axis;
% make gain analysis plot for synthetic data and linear model
[output_data] = GainAnalysis(SynData(s:z),PredictionFromStim(s:z),PID(s:z),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Model fit using stimulus','FontSize',font_size)
ylabel('Synthetic Data (a.u.)','FontSize',font_size)


% Now, we try to fit the DA model to the filtered stimulus, and see that we are close to the solution:
if ~exist('p3.mat','file')
	data.PID = fPID;
	data.f = SynData;
	x0 = [3245 34 0.3 2  2   16 1];
	lb = [3000 20 0   0  2   0  1 ];
	ub = [5000 50 1   10 10 100 10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	p3 = ValidateDAParameters(x,'ga');
	save('p3.mat','p3')

end

load p3.mat
PredictionFromFiltStim = DA_integrate(fPID,p3);
PredictionFromFiltStim = PredictionFromFiltStim - mean(PredictionFromFiltStim(100:end));

figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
multiplot(time,SynData,PredictionFromFiltStim,PredictionFromStim);
set(gca,'XLim',[15 20],'FontSize',font_size,'LineWidth',2)


% If we try to fit a DA model to some filtered version of the stimulus instead of the stimulus itself,do we lose the ability to equally predict gains in low and high stimuli areas?

s = 300; % when we start for the gain analysis
z = length(PredictionFromFiltStim); % where we end
history_lengths = [102];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(fPID(s:z)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,fPID(s:z));
	shat(i,1:hl(i)) = NaN;
end

figure('outerposition',[0 0 600 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
plot_here = axis;
% make gain analysis plot for synthetic data and linear model
[output_data] = GainAnalysis(SynData(s:z),PredictionFromFiltStim(s:z),fPID(s:z),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Prediction from Filtered Stimulus','FontSize',font_size)
ylabel('Synthetic Data (a.u.)','FontSize',font_size)


% And we can compare it over all history lengths:
s = 300; % when we start for the gain analysis
history_lengths = [30 102 150 300 600 1002 1500 2001];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(fPID(s:end)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,fPID(s:end));
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(SynData(s:end),PredictionFromStim(s:end),fPID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Prediction from actual stimulus','FontSize',font_size)
ylabel('ORN response (a.u.)','FontSize',font_size)
set(gca,'YLim',[0.7 1.5])


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(SynData(s:end),PredictionFromFiltStim(s:end),fPID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Prediction from filtered stimulus','FontSize',font_size)
legend('Location',[0.7674    0.5927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.7 1.5])



% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R(s:z),Rguessf(s:z),fstimulus(s:z),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})






% if ~exist('gap.mat','file')
% 	FitnessFunction = @(x) DA_cost_function(x,data,@Cost,'ga');
% 	gaoptions = gaoptimset('Display','iter','UseParallel',true,'TolFun',1e-6,'MigrationFraction',0.5,'ParetoFraction',0.2);
% 	lb = [1500 15 0 0 2 0 2];
% 	ub = [2000 20 1 2 10 10 10];
% 	x = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
% 	gap = ValidateDAParameters(x,'ga');
% 	save('gap.mat','gap')
% end


% % % use fminsearch to find parameters
% if ~exist('fminp','var')
% 	x0 = [0.8262   0.005  0.6   0.25    11    66    1]; 
% 	foptions = optimset('Display','iter','TolFun',1e-6,'TolX',1e-6,'MaxFunEvals',1e4);
% 	x = fminsearch(@(x) DA_cost_function(x,data,@Cost,'fminsearch'),x0,foptions);
% 	fminp = ValidateDAParameters(x,'fminsearch');
% end

% figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% fpDA = DA_integrate(PID,fminp);
% multiplot(time,f,fpDA);
% set(gca,'XLim',[20 30])
% PrettyFig;

% use pattern search to find parameters
if ~exist('psp','var')
	x0 = [1 0.25 0.1 0.09 20 60 20];
	lb = [6500 40 0 0 2 0 2];
	ub = [7000 45 1 10 10 100 10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	psp = ValidateDAParameters(x,'ga');
end

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
fpDA = DA_integrate(PID,psp);
fpDA = fpDA - mean(fpDA);
multiplot(time,f,fpDA);
set(gca,'XLim',[20 30])
PrettyFig;

% Then, choosing some arbitrary parameters for the DA model, we generate the DA model output. This is shown in the black line in the figure below (the bottom panel). Using this model output and the known input, we run a global optimisation procedure (a genetic algorithm) on this data set fifty times, and estimate the parameters that give predicted responses with lowest least square error. One such solution is shown below in red, together with a simple linear prediction from a linear filter in green.


% OK, so the genetic algorithm seems to find some parameters that do an OK job of predicting the response. How good is the estimate of the parameters of the model itself? In the following figure I plot the ensemble predictions from the 50 different realisations of the optimisation algorithm, normalised by the value of each parameter to visualse how well we can estimate the "real" parameters of the model. 
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
for i = 1:7
	x =  i*ones(1,50)+0.1*rand(1,50);
	switch i
	case 1
		y=[ensemblep.A]./p.A;
		text(i,min(y)/2,'\alpha')
	case 2
		y=[ensemblep.B]./p.B;
		text(i,min(y)/2,'\beta')
	case 3
		y=[ensemblep.C]./p.C;
		text(i,min(y)/2,'\gamma')
	case 4
		y=[ensemblep.tau_y]./p.tau_y;
		text(i,min(y)/2,'\tau_{y}')
	case 5
		y=[ensemblep.n_y]./p.n_y;
		text(i,min(y)/2,'n_{y}')
	case 6
		y=[ensemblep.tau_z]./p.tau_z;
		text(i,min(y)/2,'\tau_{z}')
	case 7
		y=[ensemblep.n_z]./p.n_z;
		text(i,min(y)/2,'n_{z}')
	end
	scatter(x,y,64)

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 7.5],'XTickLabel',{})


% Ignoring for the moment the fact that there are two clusters, and concentrating only on the better-fit cluster (the one on the right), we see no correlation between quality of fit (as measured by r-square) and the relative absolute difference between estimated parameter and actual parameter of the DA model.

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1000 700]); hold on
for i = 1:7
	x =  zeros(1,50);
	for j = 1:50
		Rguess = DA_integrate(stimulus,ensemblep(j));
		x(j) = rsquare(R,Rguess);
	end
	clear j
	switch i
	case 1
		y=abs([ensemblep.A]-p.A)./p.A;
	case 2
		y=abs([ensemblep.B]-p.B)./p.B;
	case 3
		y=abs([ensemblep.C]-p.C)./p.C;
	case 4
		y=abs([ensemblep.tau_y]-p.tau_y)./p.tau_y;
	case 5
		y=abs([ensemblep.n_y]-p.n_y)./p.n_y;
	case 6
		y=abs([ensemblep.tau_z]-p.tau_z)./p.tau_z;
	case 7
		y=abs([ensemblep.n_z]-p.n_z)./p.n_z;
	end
	scatter(x,y,64,'filled')

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 1])
xlabel('rsquare of prediction and data','FontSize',font_size)
ylabel('Relative absolute difference b/w params.','FontSize',font_size)

