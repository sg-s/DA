%% Fitting the Dynamical Adaptation Model to ORN Responses
% In this document, I will try to fit the Dynamical Adaptation (DA) model to data from random stimulation experiments on ORNs. First, the DA model is fit to synthetic data generated by the DA model, and then is fit to real data. I then go through the analysis of the instantaneous gain for a linear model and the DA model, and characterize the fast adaptation properties of ORNs to this odour. 

%% Fitting DA Model to synthetic data 
% The Dynamical Adaptation model has 8 parameters that need to be constrained. Setting one of them, $\tau_{r}$, to zero eliminates the need to solve a differential equation, and the response is given in closed form as a function of the stimulus. This greatly speeds up code execution.  

% plotting params
marker_size = 6;
marker_size2 = 32;
font_size = 24;
cfilter_length = 333;

% pick some random parameters
p.A = 1500; % alpha in model
p.B = 15; % beta in the model
p.C = 0.3; % gamma in the model
p.tau_y = 1;
p.n_y = 2;
p.tau_z = 10;
p.n_z = 2;
p.tau_r = 0; % we set tau_r to zero to start with


% random noise stimulus
stimulus = 1+(2*randn(1,11000));
stimulus = filter(ones(1,100)/100,1,stimulus);
R = DA_integrate(stimulus,p);
stimulus(1:1000) = [];
R(1:1000) = [];

% normalise R
R = R/std(R);
R = R - mean(R);

% show the stimulus and the output
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
mpo.LineWidth = 2;
mpo.Color = 'k';
mpo.font_size = 24;
ah=multiplot([],stimulus,R,mpo); hold on

% now run the GA estimation ~50 times to get a sense of how well we can extract the parameters of the model
if ~exist('sdatap.mat','file')
	% run GA
	data.PID = stimulus;
	data.f = R;
	FitnessFunction = @(x) DA_cost_function(x,data,@Cost,'ga');
	gaoptions = gaoptimset('Display','none','UseParallel',true,'TolFun',1e-6,'MigrationFraction',0.5,'ParetoFraction',0.2);
	lb = [0 0 0 0 2 0 2];
	ub = [2000 20 1 10 20 100 20];
	x = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
	sdatap = ValidateDAParameters(x,'ga');
	save('sdatap.mat','sdatap')
end

load sdatap.mat
% now find the result from the guess
Rguess = DA_integrate(stimulus,sdatap);
Rguess = Rguess/std(Rguess);
Rguess = Rguess - mean(Rguess);
plot(Rguess,'r','LineWidth',2)
% now find the linear prediction
filter_length = 333;
K = FindBestFilter(stimulus,R);
sdatafp = filter(K,1,stimulus-mean(stimulus)); 
plot(sdatafp,'g','LineWidth',2)
legend({'Response','DA Prediction','Linear Prediction'});
set(gca,'XLim',[2000 3000])
title(ah(1),'Figure 1: Fitting DA model to synthetic data')
PrettyFig;

% censor data
R = R(100:end);
Rguess = Rguess(100:end);
sdatafp=sdatafp(100:end);
stimulus = stimulus(100:end);

%%
% First, I will fit these 7 parameters to synthetic data, generated by running the DA model on filtered random noise input. In Figure 1, the top panel shows the input to the model, which is some filtered and shifted random noise. The bottom panel shows the model output (black), the linear prediction (green), and finally the DA model prediction (red).

%%
% The DA model fits the data really well; the black line of the synthetic data is completely hidden by the red line of the DA model prediction. To fit the DA model, I used a genetic algorithm to perform a nearly-global search with integer constraints on $n_{y}$ and $n_{z}$, and $\gamma$ constrained to $[0,1]$


%% Fitting DA Model to real data
% Now that we know that we can fit the model, and that our optimisation algorithm works, we will try to fit real data. The following figure shows the sample data we use. The top panel shows the stimulus as measured by the PID, and the bottom panel shows the firing rate of the ORN. The firing rate has been divided by its standard deviation and has been mean subtracted. Also shown is the linear prediction of the firing rates. 


if ~exist('PID','var')
	filename = '~/Desktop/final_2011_06_14_ab3A_1o3ol3X-3_20ml_30sec_30ms_rand.mat';
	[PID, time, f,Valve, uncropped] = PrepData3(filename);
	PID = PID(:);
	time = time(:);
	f = f(:);

	% detrend PID
	ptrend = fit(time,PID,'Poly1'); 
	PID = PID - (ptrend(time) - mean(ptrend(time)));

	% prepare data

	f = f/std(f);
	f = f(:) - mean(f);


	% assemble into a data structure
	data.PID = PID;
	data.f = f;
	data.Valve = Valve;
end


% build a simple linear model
K = FindBestFilter(PID,f);
LinearFit = filter(K,1,PID-mean(PID));

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
a=multiplot(time,PID,f,LinearFit);
title(a(1),'Figure 2: ORN data, and linear model prediction')
set(gca,'XLim',[20 25])
PrettyFig;


%%
% Now, we fit the DA model to the data using a genetic algorithm as before. I impose integer constraints on $n_{y}$ and $n_{z}$, and $\gamma$ is constrained to $[0,1]$

% use GA to find parameters
if ~exist('gap.mat','file')
	FitnessFunction = @(x) DA_cost_function(x,data,@Cost,'ga');
	gaoptions = gaoptimset('Display','iter','UseParallel',true,'TolFun',1e-6,'MigrationFraction',0.5,'ParetoFraction',0.2);
	lb = [0 0 0 0 2 0 2];
	ub = [2000 20 1 10 20 100 20];
	x = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
	gap = ValidateDAParameters(x,'ga');
	save('gap.mat','gap')
end


load gap.mat
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
DAFit = DA_integrate(PID,gap);
DAFit =  DAFit-mean(DAFit);
multiplot(time,f,DAFit,LinearFit);
title('Figure 3: Comparison of DA Model fits and linear fits to real data')
set(gca,'XLim',[20 25])
PrettyFig;



%%
% The DA model seems to do a pretty good job estimating the ORN output. Is it better than the simple linear prediction? Here, we compare the r-square and the l-2 norm between the data and the fit. For the simple linear model, the r-square is 
disp(rsquare(f(1000:end),LinearFit(1000:end)))

%% 
% The l-2 norm of the linear fit is
disp(l2(f(1000:end),LinearFit(1000:end)))

%%
% Now, for the DA model fit, the rsquare is 
disp(rsquare(f(1000:end),DAFit(1000:end)))

%% 
% The l-2 norm of the DA fit is
disp(l2(f(1000:end),DAFit(1000:end)))



% close all since we're publishing this document
close all

%% Real Data: Gain Analysis
% Sensors can exhibit fast adaptation to the stimulus, on a time-scale not dissimilar to the time-scale of response to the stimulus. In this analysis, we smooth the stimulus to the sensor over some arbitrary history window, and plot the actual response of the sensor to the model prediction for the top 10% of smoothed stimulus input, and for the bottom 10% of the smoothed stimulus input. 

%%
% In the figure below, we characterise the "fast adaptation" properties in the synthetic data.  The plot on the left compares the data (on the Y-axis) to the linear fit, while the plot on the right compares the data to the DA model fit. Several interesting features are visible: 
% 
% # The best-fit lines to the top 10% (red) and bottom 10% (red) of stimulus have different slopes in the linear fit. In particular, the response to the stronger stimuli (red) has lower gain (smaller slope) than the response to the lower stimuli (green). 
% # This is not true for the fit to the DA model. The DA model does not systematically wrongly estimate the gain of the synthetic data, unlike the linear fit. 
%


history_lengths = [30];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,sdatafp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('Synthetic Data (a.u.)','FontSize',font_size)


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R,Rguess,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})


%%
% Similarly, we can perform a similar analysis on the real data from the ORN. The plot on the left compares the ORN response data (on the Y-axis) to the linear fit, while the plot on the right compares the data to the DA model fit. 
s = 200; % when we start for the gain analysis
history_lengths = [102];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(PID(s:end)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,PID(s:end));
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(f(s:end),LinearFit(s:end),PID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('ORN response (a.u.)','FontSize',font_size)


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(f(s:end),DAFit(s:end),PID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})


%%
% In the analysis above, we have kept the "window history" length fixed at ~100ms. How does varying this window change the separation of slopes of best fit lines for the top 10% and the bottom 10%? 

history_lengths = [30 102 150 300 600 1002 1500];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 450],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,sdatafp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Linear Prediction','FontSize',font_size)
set(gca,'YLim',[0.7 1.4])

% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R,Rguess,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.7 1.4])


%%
% And we can do the same thing for the ORN response data.

s = 200; % when we start for the gain analysis
history_lengths = [30 102 150 300 600 1002 1500 2001];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(PID(s:end)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,PID(s:end));
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(f(s:end),LinearFit(s:end),PID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Linear Prediction','FontSize',font_size)
ylabel('ORN response (a.u.)','FontSize',font_size)
set(gca,'YLim',[0.7 1.3])


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(f(s:end),DAFit(s:end),PID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.7 1.3])

return
% unused code

% % % use fminsearch to find parameters
% if ~exist('fminp','var')
% 	x0 = [0.8262   0.005  0.6   0.25    11    66    1]; 
% 	foptions = optimset('Display','iter','TolFun',1e-6,'TolX',1e-6,'MaxFunEvals',1e4);
% 	x = fminsearch(@(x) DA_cost_function(x,data,@Cost,'fminsearch'),x0,foptions);
% 	fminp = ValidateDAParameters(x,'fminsearch');
% end

% figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% fpDA = DA_integrate(PID,fminp);
% multiplot(time,f,fpDA);
% set(gca,'XLim',[20 30])
% PrettyFig;

% % use pattern search to find parameters
% if ~exist('psp','var')
% 	x0 = [1 0.25 0.1 0.09 20 60 20];
% 	lb = [0.5 0 0 0 2 0 2];
% 	ub = [1.5 0.5 1 10 200 100 200];
% 	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',1000,'MaxFunEvals',10000);
% 	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
% 	psp = ValidateDAParameters(x,'ga');
% end

% figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% fpDA = DA_integrate(PID,psp);
% multiplot(time,f,fpDA);
% set(gca,'XLim',[20 30])
% PrettyFig;

% Then, choosing some arbitrary parameters for the DA model, we generate the DA model output. This is shown in the black line in the figure below (the bottom panel). Using this model output and the known input, we run a global optimisation procedure (a genetic algorithm) on this data set fifty times, and estimate the parameters that give predicted responses with lowest least square error. One such solution is shown below in red, together with a simple linear prediction from a linear filter in green.


% OK, so the genetic algorithm seems to find some parameters that do an OK job of predicting the response. How good is the estimate of the parameters of the model itself? In the following figure I plot the ensemble predictions from the 50 different realisations of the optimisation algorithm, normalised by the value of each parameter to visualse how well we can estimate the "real" parameters of the model. 
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
for i = 1:7
	x =  i*ones(1,50)+0.1*rand(1,50);
	switch i
	case 1
		y=[ensemblep.A]./p.A;
		text(i,min(y)/2,'\alpha')
	case 2
		y=[ensemblep.B]./p.B;
		text(i,min(y)/2,'\beta')
	case 3
		y=[ensemblep.C]./p.C;
		text(i,min(y)/2,'\gamma')
	case 4
		y=[ensemblep.tau_y]./p.tau_y;
		text(i,min(y)/2,'\tau_{y}')
	case 5
		y=[ensemblep.n_y]./p.n_y;
		text(i,min(y)/2,'n_{y}')
	case 6
		y=[ensemblep.tau_z]./p.tau_z;
		text(i,min(y)/2,'\tau_{z}')
	case 7
		y=[ensemblep.n_z]./p.n_z;
		text(i,min(y)/2,'n_{z}')
	end
	scatter(x,y,64)

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 7.5],'XTickLabel',{})


% Ignoring for the moment the fact that there are two clusters, and concentrating only on the better-fit cluster (the one on the right), we see no correlation between quality of fit (as measured by r-square) and the relative absolute difference between estimated parameter and actual parameter of the DA model.

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1000 700]); hold on
for i = 1:7
	x =  zeros(1,50);
	for j = 1:50
		Rguess = DA_integrate(stimulus,ensemblep(j));
		x(j) = rsquare(R,Rguess);
	end
	clear j
	switch i
	case 1
		y=abs([ensemblep.A]-p.A)./p.A;
	case 2
		y=abs([ensemblep.B]-p.B)./p.B;
	case 3
		y=abs([ensemblep.C]-p.C)./p.C;
	case 4
		y=abs([ensemblep.tau_y]-p.tau_y)./p.tau_y;
	case 5
		y=abs([ensemblep.n_y]-p.n_y)./p.n_y;
	case 6
		y=abs([ensemblep.tau_z]-p.tau_z)./p.tau_z;
	case 7
		y=abs([ensemblep.n_z]-p.n_z)./p.n_z;
	end
	scatter(x,y,64,'filled')

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 1])
xlabel('rsquare of prediction and data','FontSize',font_size)
ylabel('Relative absolute difference b/w params.','FontSize',font_size)

