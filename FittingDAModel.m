%% Fitting the Dynamical Adaptation Model to ORN Responses
% In this document, I will try to fit the Dynamical Adaptation (DA) model to data from random stimulation experiments on ORNs. First, the DA model is fit to synthetic data generated by the DA model, and then is fit to real data. I then go through the analysis of the instantaneous gain for a linear model and the DA model, and characterize the fast adaptation properties of ORNs to this odour. 

%% Synthetic Data: Fitting the DA model
% The Dynamical Adaptation model has 8 parameters that need to be constrained. Setting one of them, $\tau_{r}$, to zero eliminates the need to solve a differential equation, and the response is given in closed form as a function of the stimulus. This greatly speeds up code execution.  

% plotting params
marker_size = 6;
marker_size2 = 32;
font_size = 24;
cfilter_length = 333;

% pick some random parameters
p.A = 100; % alpha in model
p.B = 35; % beta in the model
p.C = 0.3; % gamma in the model
p.tau_y = 1;
p.n_y = 2;
p.tau_z = 10;
p.n_z = 2;
p.tau_r = 0; % we set tau_r to zero to start with



% now run the GA estimation 
if ~exist('sdatap.mat','file')
	% make the stimulus
	stimulus = exp(randn(1,11000));
	%stimulus = abs(filter(ones(1,10)/10,1,stimulus));
	R = DA_integrate(stimulus,p);
	% normalise R
	R = R/std(R);
	R = R - mean(R);
	R = R + 0.2*randn(1,length(R));

	stimulus(1:1000) = [];
	R(1:1000) = [];

	% run pattern search
	data.PID = stimulus;
	data.f = R;
	x0 = [100 35 0.3 1 2 10 2];
	lb = [10 0 0 0 2 0 2];
	ub = [500 50 1 10 10 100 10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	sdatap = ValidateDAParameters(x,'ga');

	save('sdatap.mat','sdatap','stimulus','R')
end
load sdatap.mat

% show the stimulus and the output
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
mpo.LineWidth = 2;
mpo.Color = 'k';
mpo.font_size = 24;
ah=multiplot([],stimulus,R,mpo); hold on



% now find the result from the guess
Rguess = DA_integrate(stimulus,sdatap);
Rguess = Rguess/std(Rguess);
Rguess = Rguess - mean(Rguess);
plot(Rguess,'r','LineWidth',2)
% now find the linear prediction
filter_length = 333;
K = FindBestFilter(stimulus,R);
sdatafp = filter(K,1,stimulus-mean(stimulus)); 
plot(sdatafp,'g','LineWidth',2)
legend({'Response','DA Prediction','Linear Prediction'});
set(gca,'XLim',[2000 2500])
title(ah(1),'Figure 1: Fitting DA model to synthetic data')
PrettyFig;

% censor data
R = R(100:end);
Rguess = Rguess(100:end);
sdatafp=sdatafp(100:end);
stimulus = stimulus(100:end);


%%
% First, I will fit these 7 parameters to synthetic data, generated by running the DA model on exponentiated and filtered gaussian random noise input. In Figure 1, the top panel shows the input to the model. The bottom panel shows the model output (black), the linear prediction (green), and finally the DA model prediction (red).

%%
% The DA model fits the data really well; the black line of the synthetic data is completely hidden by the red line of the DA model prediction. To fit the DA model, I used a genetic algorithm to perform a nearly-global search with integer constraints on $n_{y}$ and $n_{z}$, and $\gamma$ constrained to $[0,1]$

%%
% The DA model seems to do a pretty good job estimating the data output. Is it better than the simple linear prediction? Here, we compare the r-square and the l-2 norm between the data and the fit. For the simple linear model, the r-square is 
disp(rsquare(R(1000:end),sdatafp(1000:end)))

%%
% cf. the DA model fit, the rsquare is 
disp(rsquare(R(1000:end),Rguess(1000:end)))

%% 
% The l-2 norm of the linear fit is
disp(l2(R(1000:end),sdatafp(1000:end)))

%% 
% cf. l-2 norm of the DA fit is
disp(l2(R(1000:end),Rguess(1000:end)))

%%
% The DA model fits the data much better, which makes sense as the stimulus is non-gaussian, and we have crafted the synthetic data set so that the DA model does the best job explaining its variance. 

%% Synthetic Data: Gain Analysis
% Sensors can exhibit fast adaptation to the stimulus, on a time-scale not dissimilar to the time-scale of response to the stimulus. In this analysis, we smooth the stimulus to the sensor over some arbitrary history window, and plot the actual response of the sensor to the model prediction for the top 10% of smoothed stimulus input, and for the bottom 10% of the smoothed stimulus input. 

%%
% In the figure below, we characterise the "fast adaptation" properties in the synthetic data.  The plot on the left compares the data (on the Y-axis) to the linear fit, while the plot on the right compares the data to the DA model fit. Several interesting features are visible: 


%% 
% # The best-fit lines to the top 10% (red) and bottom 10% (red) of stimulus have different slopes in the linear fit. In particular, the response to the stronger stimuli (red) has lower gain (smaller slope) than the response to the lower stimuli (green). 
% # This is not true for the fit to the DA model. The DA model does not systematically wrongly estimate the gain of the synthetic data, unlike the linear fit. 


history_lengths = [30];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,sdatafp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('Synthetic Data (a.u.)','FontSize',font_size)


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R,Rguess,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})


%%
% How does this vary with the length of the history window?  In the analysis above, we have kept the "window history" length fixed. How does varying this window change the separation of slopes of best fit lines for the top 10% and the bottom 10%? 

history_lengths = [30 102 150 300 600 1002 1500];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(stimulus));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,stimulus);
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 450],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(R,sdatafp,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Linear Prediction','FontSize',font_size)
set(gca,'YLim',[0.6 1.7])

% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(R,Rguess,stimulus,shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.6 1.7])


%% Fitting DA Model to real data
% Now that we know that we can fit the model, and that our optimisation algorithm works, we will try to fit real data. The following figure shows the sample data we use. The top panel shows the stimulus as measured by the PID, and the bottom panel shows the firing rate of the ORN. The firing rate has been divided by its standard deviation and has been mean subtracted. Also shown is the linear prediction of the firing rates. 


if ~exist('PID','var')
	filename = '/Volumes/Data/random-stim/final_2011_06_14_ab3A_1o3ol3X-3_20ml_30sec_30ms_rand.mat';
	[PID, time, f,Valve, uncropped] = PrepData3(filename);
	PID = PID(:);
	time = time(:);
	f = f(:);

	% detrend PID
	% ptrend = fit(time,PID,'Poly1'); 
	% PID = PID - (ptrend(time) - mean(ptrend(time)));

	% prepare data

	f = f/std(f);
	f = f(:) - mean(f);


	% assemble into a data structure
	data.PID = PID;
	data.f = f;
	data.Valve = Valve;
end


% build a simple linear model
K = FindBestFilter(PID,f,[],'min_cutoff=min(response);');
LinearFit = filter(K,1,PID-mean(PID));
LinearFit(LinearFit<min(f)) = min(f);

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
a=multiplot(time,PID,f,LinearFit);
title(a(1),'Figure 2: ORN data, and linear model prediction')
set(gca,'XLim',[20 25])
PrettyFig;


%%
% Now, we fit the DA model to the data using a pattern search optimisation (the type of optimisation doesn't matter. Pattern search is faster and converges to local minima faster than GA). $\gamma$ is constrained to $[0,1]$

% use pattern search to find parameters
if ~exist('psp.mat','file')
	x0 = [4200  50   0.2 2.6 1.4   30   0.1];
	lb = [3500  40   0   0   0     20   0];
	ub = [5200  60   1   10  5     50   10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	psp = ValidateDAParameters(x,'ga');
	save('psp.mat','psp')
end

load psp.mat
figure('outerposition',[0 0 1400 500],'PaperUnits','points','PaperSize',[1400 500]); hold on
DAFit = DA_integrate(PID,psp);
DAFit = DAFit - mean(DAFit(500:end));
DAFit(DAFit<min(f))=min(f);

mpo = [];
mpo.legend = 1;

subplot(1,3,1), hold on
multiplot(time,f,LinearFit,DAFit,mpo);
set(gca,'XLim',[min(time)+1 min(time)+3])
PrettyFig;

mpo.legend = 0;

subplot(1,3,2),  hold on
multiplot(time,f,LinearFit,DAFit,mpo);
set(gca,'XLim',[mean(time)-1 mean(time)+1])
set(gca,'YTick',[])
set(gca,'YColor','w')
PrettyFig;


subplot(1,3,3), hold on
multiplot(time,f,LinearFit,DAFit,mpo);
set(gca,'XLim',[max(time)-2 max(time)])
set(gca,'YTick',[])
set(gca,'YColor','w')
PrettyFig;





%%
% The DA model seems to do a pretty good job estimating the ORN output. Is it better than the simple linear prediction? Here, we compare the r-square and the l-2 norm between the data and the fit. For the simple linear model, the r-square is 
s = 500;
disp(rsquare(f(s:end),LinearFit(s:end)))

%%
% cf. for the DA model fit, the rsquare is 
disp(rsquare(f(s:end),DAFit(s:end)))

%% 
% The l-2 norm of the linear fit is
disp(l2(f(s:end),LinearFit(s:end)))

%% 
% cf. l-2 norm of the DA fit is
disp(l2(f(s:end),DAFit(s:end)))



%% Real Data: Gain Analysis
% We can perform a similar gain analysis like we did on the synthetic data on the real data from the ORN. The plot on the left compares the ORN response data (on the Y-axis) to the linear fit, while the plot on the right compares the data to the DA model fit. 
s = 500; % when we start for the gain analysis
z = length(f); % where we end
history_lengths = [102];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(PID(s:z)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,PID(s:z));
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(f(s:z),LinearFit(s:z),PID(s:z),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('Linear Prediction','FontSize',font_size)
ylabel('ORN response (a.u.)','FontSize',font_size)

% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(f(s:z),DAFit(s:z),PID(s:z),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,1,plot_here);
xlabel('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})



%%
% In the analysis above, we have kept the "window history" length fixed at ~100ms. How does varying this window change the separation of slopes of best fit lines for the top 10% and the bottom 10%? 



%%
% And we can do the same thing for the ORN response data.

s = 500; % when we start for the gain analysis
history_lengths = [30 102 150 300 600 1002 1500 2001];
hl = history_lengths/3; % history lengths better be divisible by 3!
shat = NaN(length(hl),length(PID(s:end)));
for i = 1:length(hl)
	shat(i,:) = filtfilt(ones(1,hl(i))/hl(i),1,PID(s:end));
	shat(i,1:hl(i)) = NaN;
end


figure('outerposition',[0 0 900 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% make gain analysis plot for synthetic data and linear model
plot_here=subplot(1,2,1); hold on
[output_data] = GainAnalysis(f(s:end),LinearFit(s:end),PID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('Linear Prediction','FontSize',font_size)
ylabel('ORN response (a.u.)','FontSize',font_size)
set(gca,'YLim',[0.7 1.3])


% make gain analysis plot for synthetic data and DA model
plot_here=subplot(1,2,2); hold on
[output_data] = GainAnalysis(f(s:end),DAFit(s:end),PID(s:end),shat,history_lengths,hl,filter_length,marker_size,marker_size2,font_size,2,plot_here);
title('DA Prediction','FontSize',font_size)
legend('Location',[0.7674    0.2927    0.21    0.1370],{'all data','bottom 10%','top 10%'})
set(gca,'YLim',[0.7 1.3])

return
% unused code

% if ~exist('gap.mat','file')
% 	FitnessFunction = @(x) DA_cost_function(x,data,@Cost,'ga');
% 	gaoptions = gaoptimset('Display','iter','UseParallel',true,'TolFun',1e-6,'MigrationFraction',0.5,'ParetoFraction',0.2);
% 	lb = [1500 15 0 0 2 0 2];
% 	ub = [2000 20 1 2 10 10 10];
% 	x = ga(FitnessFunction,7,[],[],[],[],lb,ub,[],[5 7],gaoptions);
% 	gap = ValidateDAParameters(x,'ga');
% 	save('gap.mat','gap')
% end


% % % use fminsearch to find parameters
% if ~exist('fminp','var')
% 	x0 = [0.8262   0.005  0.6   0.25    11    66    1]; 
% 	foptions = optimset('Display','iter','TolFun',1e-6,'TolX',1e-6,'MaxFunEvals',1e4);
% 	x = fminsearch(@(x) DA_cost_function(x,data,@Cost,'fminsearch'),x0,foptions);
% 	fminp = ValidateDAParameters(x,'fminsearch');
% end

% figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
% fpDA = DA_integrate(PID,fminp);
% multiplot(time,f,fpDA);
% set(gca,'XLim',[20 30])
% PrettyFig;

% use pattern search to find parameters
if ~exist('psp','var')
	x0 = [1 0.25 0.1 0.09 20 60 20];
	lb = [6500 40 0 0 2 0 2];
	ub = [7000 45 1 10 10 100 10];
	psoptions = psoptimset('UseParallel',true,'CompletePoll', 'on', 'Vectorized', 'off','Display','iter','MaxIter',2000,'MaxFunEvals',10000);
	x = patternsearch(@(x) DA_cost_function(x,data,@Cost,'ga'),x0,[],[],[],[],lb,ub,psoptions);
	psp = ValidateDAParameters(x,'ga');
end

figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
fpDA = DA_integrate(PID,psp);
fpDA = fpDA - mean(fpDA);
multiplot(time,f,fpDA);
set(gca,'XLim',[20 30])
PrettyFig;

% Then, choosing some arbitrary parameters for the DA model, we generate the DA model output. This is shown in the black line in the figure below (the bottom panel). Using this model output and the known input, we run a global optimisation procedure (a genetic algorithm) on this data set fifty times, and estimate the parameters that give predicted responses with lowest least square error. One such solution is shown below in red, together with a simple linear prediction from a linear filter in green.


% OK, so the genetic algorithm seems to find some parameters that do an OK job of predicting the response. How good is the estimate of the parameters of the model itself? In the following figure I plot the ensemble predictions from the 50 different realisations of the optimisation algorithm, normalised by the value of each parameter to visualse how well we can estimate the "real" parameters of the model. 
figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
for i = 1:7
	x =  i*ones(1,50)+0.1*rand(1,50);
	switch i
	case 1
		y=[ensemblep.A]./p.A;
		text(i,min(y)/2,'\alpha')
	case 2
		y=[ensemblep.B]./p.B;
		text(i,min(y)/2,'\beta')
	case 3
		y=[ensemblep.C]./p.C;
		text(i,min(y)/2,'\gamma')
	case 4
		y=[ensemblep.tau_y]./p.tau_y;
		text(i,min(y)/2,'\tau_{y}')
	case 5
		y=[ensemblep.n_y]./p.n_y;
		text(i,min(y)/2,'n_{y}')
	case 6
		y=[ensemblep.tau_z]./p.tau_z;
		text(i,min(y)/2,'\tau_{z}')
	case 7
		y=[ensemblep.n_z]./p.n_z;
		text(i,min(y)/2,'n_{z}')
	end
	scatter(x,y,64)

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 7.5],'XTickLabel',{})


% Ignoring for the moment the fact that there are two clusters, and concentrating only on the better-fit cluster (the one on the right), we see no correlation between quality of fit (as measured by r-square) and the relative absolute difference between estimated parameter and actual parameter of the DA model.

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1000 700]); hold on
for i = 1:7
	x =  zeros(1,50);
	for j = 1:50
		Rguess = DA_integrate(stimulus,ensemblep(j));
		x(j) = rsquare(R,Rguess);
	end
	clear j
	switch i
	case 1
		y=abs([ensemblep.A]-p.A)./p.A;
	case 2
		y=abs([ensemblep.B]-p.B)./p.B;
	case 3
		y=abs([ensemblep.C]-p.C)./p.C;
	case 4
		y=abs([ensemblep.tau_y]-p.tau_y)./p.tau_y;
	case 5
		y=abs([ensemblep.n_y]-p.n_y)./p.n_y;
	case 6
		y=abs([ensemblep.tau_z]-p.tau_z)./p.tau_z;
	case 7
		y=abs([ensemblep.n_z]-p.n_z)./p.n_z;
	end
	scatter(x,y,64,'filled')

end
clear i
PrettyFig;
set(gca,'YScale','log','XLim',[0.5 1])
xlabel('rsquare of prediction and data','FontSize',font_size)
ylabel('Relative absolute difference b/w params.','FontSize',font_size)

