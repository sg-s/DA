pHeader;


%% Is Adaptation distinct from receptor saturation?
% First, we constructed a nonlinear-linear-nonlinear model (a-c). (a) The input nonlinearity is a Hill function, with a $K_D$ chosen so that the stimuli we used straddle it. (b) shows the filter used in this model, which is the filter calculated from ORNs stimulated with Gaussian noise inputs. (c) is a simple rectifying output nonlinearity, since ORN responses cannot be negative. (d-f) Analysis of response generated by this model to naturalistic stimulus from Fig. 2, identical to how we analyzed the real data. (d) Filter estimated from nat. stim. (red) compared to actual filter in model. (e) Comparison of model response to stimulus projected through the reconstructed filter. (f) Per-whiff gain varies non-monotonically with the mean stimulus in the preceding 300 ms (cf. Fig 2e). (g-i) Analysis of NLN model response to stimuli with increasing mean from Fig. 1, identical to how we analyzed the real data. (g) Filters reliably extracted. Colors correspond to mean stimulus (as in Fig. 1). (h) Comparison of model responses to stimulus projected through reconstructed filters on a trial-wise basis. Note that all curves lie along a single curve (cf. Fig. 1e) (i) Gain vs. mean stimulus. 


% get the filter from the Gaussian stimuli 
clear MSGdata
MSGdata = consolidateData2(getPath(dataManager,'93ba5d68174e3df9f462a1fc48c581da'));
MSGdata = cleanMSGdata(MSGdata);
MSG_filter = MSGdata.K2(:,MSGdata.paradigm==1);
MSGdata.K = mean(MSG_filter(:,[2 5]),2);
MSGdata.filtertime = 1e-3*(1:length(MSGdata.K)) - .1;

% get the naturalistic stimuli 
clear ab3 ab2
load(getPath(dataManager,'5c7dacc5b42ff0eebb980d80fec120c3'),'data','spikes')
PID = data(2).PID;
time = 1e-4*(1:length(PID));
all_spikes = spikes(2).A;

% A spikes --> firing rate
fA = spiketimes2f(all_spikes,time);

tA = 1e-3*(1:length(fA));
PID2 = fA;
for i = 1:width(PID2)
	PID2(:,i) = interp1(time,PID(i,:),tA);
end
PID = PID2; clear PID2
% some minor cleaning up
PID(end,:) = PID(end-1,:); 

% remove the baseline from the PID, and remember the error
PID_baseline = mean(mean(PID(1:5e3,:)));
PID = PID - PID_baseline;

NSdata.fA = mean(fA,2);
NSdata.PID = mean(PID,2);
NSdata.time = 1e-3*(1:length(NSdata.fA));

% generate an input nonlinearity 

all_k_d = [1e-1 5e-1 1 10];


for ki = 1:length(all_k_d)

	hill_param = [1 all_k_d(ki) 1];
	x = logspace(-2,1,100);
	H = hill(hill_param,x);


	figure('outerposition',[0 0 901 899],'PaperUnits','points','PaperSize',[901 899]); hold on
	clear ax
	for i = 9:-1:1
		ax(i) = subplot(3,3,i); hold on
	end


	plot(ax(1),x,H,'k')
	xlabel(ax(1),'Stimulus (a.u.)')
	ylabel(ax(1),'H(s)')
	set(ax(1),'XScale','log','YScale','linear')


	plot(ax(2),MSGdata.filtertime,MSGdata.K,'k')
	xlabel(ax(2),'Filter lag (s)')
	ylabel(ax(2),'Filter amplitude (a.u.)')

	x = linspace(-1,1,100);
	y = x; y(x<0) = 0;
	plot(ax(3),x,y,'k')
	xlabel(ax(3),'x(t)')
	ylabel(ax(3),'R(t)')


	% first do nat. stim. -- play the stimulus to the NL model. 
	history_length = 300;
	y = hill(hill_param,NSdata.PID);
	NSdata.NL_pred = convolve(NSdata.time,y,MSGdata.K,MSGdata.filtertime);
	NSdata.NL_pred(NSdata.NL_pred<0) = 0;

	% now fit a filter to this data
	NSdata.Khat = fitFilter2Data(NSdata.PID,NSdata.NL_pred,'reg',1,'offset',100,'filter_length',700);

	% compare reconstructed filter to actual filter
	plot(ax(4),MSGdata.filtertime,MSGdata.K,'k')
	xlabel(ax(4),'Filter lag (s)')
	ylabel(ax(4),'Filter amplitude (a.u.)')
	plot(ax(4),MSGdata.filtertime,NSdata.Khat,'r')
	legend(ax(4),'Actual filter','reconstructed filter')

	% reproject
	NSdata.K_pred = convolve(NSdata.time,NSdata.PID,NSdata.Khat,MSGdata.filtertime);

	shat = computeSmoothedStimulus(NSdata.PID,history_length);
	shat = shat-min(shat);
	shat = shat/max(shat);
	shat = 1+ceil(shat*99);
	shat(isnan(shat)) = 1;

	% make the output analysis plot
	axes(ax(5))
	cc = parula(100);
	c = cc(shat,:);
	scatter(NSdata.K_pred,NSdata.NL_pred,20,c,'filled')

	shat = computeSmoothedStimulus(NSdata.PID,history_length);
	ch = colorbar('east');
	ch.Position = [0.582    0.45    0.01    0.06];
	caxis([min(shat) max(shat)]);

	xlabel(ax(5),'Projected Stimulus (a.u.)')
	ylabel(ax(5),'NL model response (a.u.)')



	% find all excursions (defined as firing rate crossing 10Hz)
	[whiff_starts,whiff_ends] = findWhiffs(NSdata.PID);

	d = finddelay(NSdata.PID(58e3:65e3),NSdata.NL_pred(58e3:65e3));
	whiff_starts = whiff_starts + d;
	whiff_ends = whiff_ends + d;

	% filter the stimulus using a box filter
	shat_mean = computeSmoothedStimulus(NSdata.PID,history_length);

	% for each excursion, estimate mean, std stimulus, gain, etc. 
	mean_stim = NaN*whiff_ends;
	gain = NaN*whiff_ends;
	gain_err =  NaN*whiff_ends;
	for i = 1:length(whiff_ends)
		mean_stim(i) = mean(shat_mean(whiff_starts(i):whiff_ends(i)));
		ff = fit(NSdata.K_pred(whiff_starts(i):whiff_ends(i)),NSdata.NL_pred(whiff_starts(i):whiff_ends(i)),'poly1');
		gain(i) = ff.p1;
		temp = confint(ff);
		gain_err(i) = diff(temp(:,1))/2;
	end
	rm_this = (abs(gain_err./gain)) > .5; % throw out points where the estimate of gain has a more than 50% error
	gain(rm_this) = [];
	gain_err(rm_this) = [];
	mean_stim(rm_this) = [];
	whiff_ends(rm_this) = [];
	whiff_starts(rm_this) = [];


	plot(ax(6),mean_stim,gain,'k+')
	xlabel(ax(6),'Mean stimulus in preceding 300ms (V)')
	ylabel(ax(6),'Gain in whiff (a.u.)')
	set(ax(6),'XScale','log','YScale','log')


	% now do the MSG 

	% ##     ##  ######   ######   
	% ###   ### ##    ## ##    ##  
	% #### #### ##       ##        
	% ## ### ##  ######  ##   #### 
	% ##     ##       ## ##    ##  
	% ##     ## ##    ## ##    ##  
	% ##     ##  ######   ######   


	% average stimulus for each paradigm value, and then play that through the model
	MSGdata.time = 1e-3*(1:length(MSGdata.PID));
	for i = 1:length(MSGdata.paradigm)
		MSGdata.NL_pred(:,i) = hill(hill_param,MSGdata.PID(:,i));
		MSGdata.NL_pred(:,i) = convolve(MSGdata.time,MSGdata.NL_pred(:,i),MSGdata.K,MSGdata.filtertime);
		MSGdata.NL_pred(MSGdata.NL_pred(:,i)<0,i) = 0;
	end

	% recover filters for every trial
	for i = 1:length(MSGdata.paradigm)
		MSGdata.Khat(:,i) = fitFilter2Data(MSGdata.PID(35e3:55e3,i),MSGdata.NL_pred(35e3:55e3,i),'reg',1,'offset',100,'filter_length',700);
		MSGdata.Khat(:,i) = MSGdata.Khat(:,i)/norm(MSGdata.Khat(:,i));
		MSGdata.Khat(:,i) = MSGdata.Khat(:,i)*norm(MSGdata.K);
	end


	% compare reconstructed filter to actual filter
	plot(ax(7),MSGdata.filtertime,MSGdata.K,'k')
	xlabel(ax(7),'Filter lag (s)')
	ylabel(ax(7),'Filter amplitude (a.u.)')
	c = parula(11);
	for i = 1:max(MSGdata.paradigm)
		plot(ax(7),MSGdata.filtertime,mean(MSGdata.Khat(:,MSGdata.paradigm==i),2),'Color',c(i,:))
	end

	% reproject all the stimuli
	for i = 1:length(MSGdata.paradigm)
		MSGdata.K_pred(:,i) = convolve(MSGdata.time,MSGdata.PID(:,i),MSGdata.Khat(:,i),MSGdata.filtertime);
	end

	% make i/o curves for different mean stimuli and compute gain in each paradigm 
	axes(ax(8)), hold(ax(8),'on')
	for i = 1:max(MSGdata.paradigm)
		x = mean(MSGdata.K_pred(35e3:55e3,MSGdata.paradigm==i),2);
		y = mean(MSGdata.NL_pred(35e3:55e3,MSGdata.paradigm==i),2);

		% s = nanmean(MSGdata.PID(:,MSGdata.paradigm==i),2);
		% x = x - nanmean(x);
		% x = x + nanmean(nanmean(s));

		plotPieceWiseLinear(x,y,'nbins',50,'Color',c(i,:),'show_error',false,'LineWidth',2);
		
	end
	xlabel(ax(8),'Projected Stimulus (a.u.)')
	ylabel(ax(8),'R (a.u.)')

	for i = 1:length(MSGdata.paradigm)
		x = MSGdata.K_pred(35e3:55e3,i);
		y = MSGdata.NL_pred(35e3:55e3,i);
		ff = fit(x(:),y(:),'poly1');
		MSGdata.NL_gain(i) = ff.p1;
	end

	mean_stim = mean(MSGdata.PID(35e3:55e3,:));
	for i = 1:max(MSGdata.paradigm)
		plot(ax(9),mean_stim(MSGdata.paradigm == i), MSGdata.NL_gain(MSGdata.paradigm==i),'+','Color',c(i,:))
	end
	set(ax(9),'XScale','log','YScale','log','XLim',[.1 2],'YLim',[1e-2 2],'XTick',[.1 .2 .5 1])
	xlabel(ax(9),'Mean stimulus (V)')
	ylabel(ax(9),'Gain (a.u.)')

	prettyFig('fs',12)

	labelFigure

	suptitle(['k_D = ' oval(all_k_d(ki))])

	drawnow

	if being_published	
		snapnow	
		delete(gcf)
	end

end

%% 
% What do we have to do make the input-output curves in this model look like what the data looks like, for the Gaussian stimuli? One possibility is that there actually is adaptation in the Gaussian stimuli, and that if we change the input nonlinearity in the NLN model, we get curves looking like what our data looks like. That's what I do in this figure: I manually set the $K_D$ of the input nonlinearity to be the mean stimulus for each paradigm, and carry out the analysis as before.  

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
clear ax
for i = 1:3
	ax(i) = subplot(1,3,i); hold on
end

% average stimulus for each paradigm value, and then play that through the model
MSGdata.time = 1e-3*(1:length(MSGdata.PID));
hill_param = [1 .5 1];
for i = 1:length(MSGdata.paradigm)
	hill_param(2) = mean(MSGdata.PID(35e3:55e3,i));
	MSGdata.NL_pred(:,i) = hill(hill_param,MSGdata.PID(:,i));
	MSGdata.NL_pred(:,i) = convolve(MSGdata.time,MSGdata.NL_pred(:,i),MSGdata.K,MSGdata.filtertime);
	MSGdata.NL_pred(MSGdata.NL_pred(:,i)<0,i) = 0;
end

% show these changing input nonlinearities 
c = parula(11);
for i = 1:max(MSGdata.paradigm)
	hill_param(2) = mean(mean(MSGdata.PID(35e3:55e3,MSGdata.paradigm==i)));
	x = logspace(-2,2,100);
	H = hill(hill_param,x); 
	plot(ax(1),x,H,'Color',c(i,:))
end
set(ax(1),'XScale','log')

% recover filters for every trial
for i = 1:length(MSGdata.paradigm)
	MSGdata.Khat(:,i) = fitFilter2Data(MSGdata.PID(35e3:55e3,i),MSGdata.NL_pred(35e3:55e3,i),'reg',1,'offset',100,'filter_length',700);
	MSGdata.Khat(:,i) = MSGdata.Khat(:,i)/norm(MSGdata.Khat(:,i));
	MSGdata.Khat(:,i) = MSGdata.Khat(:,i)*norm(MSGdata.K);
end


% reproject all the stimuli
for i = 1:length(MSGdata.paradigm)
	MSGdata.K_pred(:,i) = convolve(MSGdata.time,MSGdata.PID(:,i),MSGdata.Khat(:,i),MSGdata.filtertime);
end

% make i/o curves for different mean stimuli and compute gain in each paradigm 
axes(ax(2)), hold(ax(2),'on')
for i = 1:max(MSGdata.paradigm)
	x = mean(MSGdata.K_pred(35e3:55e3,MSGdata.paradigm==i),2);
	y = mean(MSGdata.NL_pred(35e3:55e3,MSGdata.paradigm==i),2);
	plotPieceWiseLinear(x,y,'nbins',50,'Color',c(i,:),'show_error',false,'LineWidth',2);
	
end
xlabel(ax(2),'Projected Stimulus (a.u.)')
ylabel(ax(2),'R (a.u.)')

for i = 1:length(MSGdata.paradigm)
	x = MSGdata.K_pred(35e3:55e3,i);
	y = MSGdata.NL_pred(35e3:55e3,i);
	ff = fit(x(:),y(:),'poly1');
	MSGdata.NL_gain(i) = ff.p1;
end

mean_stim = mean(MSGdata.PID(35e3:55e3,:));
for i = 1:max(MSGdata.paradigm)
	plot(ax(3),mean_stim(MSGdata.paradigm == i), MSGdata.NL_gain(MSGdata.paradigm==i),'+','Color',c(i,:))
end
set(ax(3),'XScale','log','YScale','log','XLim',[.1 2],'YLim',[1e-2 2],'XTick',[.1 .2 .5 1])
xlabel(ax(3),'Mean stimulus (V)')
ylabel(ax(3),'Gain (a.u.)')

prettyFig('fs',12)

suptitle('Changing K_D of input nonlinearity in NLN model reproduces features of data')

drawnow

if being_published	
	snapnow	
	delete(gcf)
end

%% Estimating the input nonlinearity directly from the data
% In this section, I attempt to directly estimate the input nonlinearity as follows: first, I swap the stimulus and the response from the NLN model data (see prev. section) and extract a filter that when convolved with the response, predicts the stimulus. This filter is mostly a-causal. In the following figure, I first plot the filter from the response to the stimulus. Note that most of the filter is in negative time (the future). Next, I plot the predicted stimulus vs. the actual stimulus (which is the estimate of the input nonlinearity). I also plot the actual input nonlinearity in red (rescaled to fit the max of the reconstructed nonlinearity). 


hill_param = [1 .5 1];
y = hill(hill_param,NSdata.PID);
NSdata.NL_pred = convolve(NSdata.time,y,MSGdata.K,MSGdata.filtertime);
% NSdata.NL_pred(NSdata.NL_pred<0) = 0;

% treat the stimulus and the response as each other, and flip time

Krev = fitFilter2Data(NSdata.NL_pred,NSdata.PID,'reg',1,'filter_length',1e3,'offset',600);

% Krev = flipud(Krev(1:700));
filtertime = 1e-3*(1:length(Krev)) - .6;

rev_proj = convolve(NSdata.time,NSdata.NL_pred,Krev,filtertime);


figure('outerposition',[0 0 1000 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
subplot(1,2,1); hold on
plot(filtertime,Krev,'k')
title('Filter from Response to Stimulus')
xlabel('Filter lag (s)')

subplot(1,2,2); hold on
plot(NSdata.PID,rev_proj,'k.')
xlabel('Actual Stimulus (V)')
ylabel('Predicted Stimulus (a.u.)')
title('Reconstructed input nonlinearity')

x = logspace(-4,log10(8),100);
H = hill(hill_param,x); H = H/max(H); H = H*max(rev_proj);
l = plot(x,H,'r');

legend(l,'Actual NL, rescaled','Location','northwest')

set(gca,'XScale','log')

prettyFig('fs',16)

labelFigure


if being_published	
	snapnow	
	delete(gcf)
end

%%
% In a perfect world, this method would show if the input nonlinearity is changing or not. It's hard to tell in this case. 

%% Fitting a NLN model to the data
% In this section, we fit a NLN model to the naturalistic stimulus data to see how well we can reproduce the data. The NLN model has a simple Hill function (n=1) as the input nonlinearity, with a fixed $K_D$. The output nonlinearity is simply a threshold linear function, and the linear filter is non-parametrically fit. Thus the only free parameter is the $K_D$. 

clear p data
data.stimulus = [NSdata.PID NSdata.fA];
p.k_D = 0.6565;
p.n = 1;
[R,K] = NLNmodel(data.stimulus,p);

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
subplot(1,3,1:2); hold on
plot(NSdata.time,NSdata.fA,'k')
plot(NSdata.time,R,'r');
legend({'Data',['NLN model, r^2 = ' oval(rsquare(R,NSdata.fA))]},'Location','northwest')
xlabel('Time (s)')
ylabel('Firing rate (Hz)')
set(gca,'XLim',[0 70])

subplot(1,3,3); hold on
plot(R,NSdata.fA,'k')
xlabel('NLN prediction (Hz)')
ylabel('ORN response (Hz)')

prettyFig();

if being_published
	snapnow
	delete(gcf)
end

return

%%
% This looks like the NLN model can reproduce the naturalistic stimulus data well. What if we fit the NLN model to the denser naturalistic stimulus data? For this dataset, we expand the model to also fit the steepness of the static input nonlinearity. This model can reproduce responses to even the dense naturalistic stimuli very well. Note that it underestimates the responses to the first two whiffs, but progressively gets better. This could be a way to estimate the timescale of gain control from this dataset. 

load('/local-data/DA-paper/data-for-paper/fig7/nat-stim-ab3/combined_data.ORNData','-mat')

clear data p
data.response = mean(od(3).firing_rate,2);
data.stimulus = [mean(od(3).stimulus,2) mean(od(3).firing_rate,2)];
data.stimulus(:,1) = data.stimulus(:,1) -min(min(od(3).stimulus(1:5e3,:)));
p.k_D = 0.0797;
p.n = 1.7561;
[R,K] = NLNmodel(data.stimulus,p);


figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
subplot(1,3,1:2); hold on
plot(NSdata.time,data.response,'k')
plot(NSdata.time,R,'r');
legend({'Data',['NLN model, r^2 = ' oval(rsquare(R(10e3:60e3),data.response(10e3:60e3)))]},'Location','northwest')
xlabel('Time (s)')
ylabel('Firing rate (Hz)')
set(gca,'XLim',[0 70])

subplot(1,3,3); hold on
plot(R,data.response,'k')
xlabel('NLN prediction (Hz)')
ylabel('ORN response (Hz)')

prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%%
% How sensitive is this to the two parameters? 

all_n = linspace(.5,4,8);
all_k_d = logspace(-3,1,10);
r2 = NaN(length(all_n),length(all_k_d));
if exist('NLN_nat_stim_v2_r2.mat','file')
	load('NLN_nat_stim_v2_r2.mat','r2')
else
	for i = 1:length(all_n)
		if ~being_published
			textbar(i,length(all_n))
		end
		for j = 1:length(all_k_d)
			clear p
			p.k_D = all_k_d(j);
			p.n = all_n(i);
			R = NLNmodel(data.stimulus,p);
			r2(i,j) = rsquare(R(10e3:60e3),data.response(10e3:60e3));
		end
		save('NLN_nat_stim_v2_r2.mat','r2')
	end
end

k_d_labels = {};
for i = 1:length(all_k_d)
	k_d_labels{i} = oval(all_k_d(i));
end


n_labels = {};
for i = 1:length(all_n)
	n_labels{i} = oval(all_n(i));
end

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1000 500]); hold on
imagesc(r2)
xlabel('K_D')
ylabel('n')
set(gca,'XTick',[1:10],'XTickLabel',k_d_labels,'XTickLabelRotation',45)
set(gca,'YTick',[1:8],'YTickLabel',n_labels)
set(gca,'XLim',[.6 10.5],'YLim',[.5 8.5])
colorbar
caxis([0.3 1])

prettyFig();

if being_published
	snapnow
	delete(gcf)
end


%%
% In comparison, I then fit a DA model to the same data, to see how well that does. 

clear p
p.   s0 = -0.0234;
p.  n_z = 2.0938;
p.tau_z = 98.4531;
p.  n_y = 3;
p.tau_y = 10.7500;
p.    C = 0.7219;
p.    A = 897.1875;
p.    B = 11.4062;
R = DAModelv2(data.stimulus(:,1),p);

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
subplot(1,3,1:2); hold on
plot(NSdata.time,data.response,'k')
plot(NSdata.time,R,'r');
legend({'Data',['DA model, r^2 = ' oval(rsquare(R(10e3:60e3),data.response(10e3:60e3)))]},'Location','northwest')
xlabel('Time (s)')
ylabel('Firing rate (Hz)')
set(gca,'XLim',[0 70],'YLim',[0 120])

subplot(1,3,3); hold on
plot(R(10e3:60e3),data.response(10e3:60e3),'k')
xlabel('DA prediction (Hz)')
ylabel('ORN response (Hz)')

prettyFig();

if being_published
	snapnow
	delete(gcf)
end


%%
% What if we allow the $k_D$ of the input nonlinearity to vary with the stimulus, and then fit this adapting NLN model to this data? In the following section, I fit a adapting NLN model to the Gaussian data, where the $k_D$ is a filtered version of the stimulus. This allows the $k_D$ to dynamically vary with the stimulus, and gives you "true" gain control and Weber's Law for free. 

clear data
use_these = [1 3 7];
for i = 1:length(use_these)
	j = use_these(i);
	S = MSGdata.PID(:,MSGdata.paradigm==j);
	R = MSGdata.fA(:,MSGdata.paradigm==j);
	rm_this = sum(R)==0;
	R(:,rm_this) = [];
	S(:,rm_this) = [];
	data(i).response = mean(R,2);
	data(i).stimulus = mean(S,2);
end

clear p
p.  k0 = 0.0537;
p. tau = 156;
p.   B = 1.1250;
p.tau1 = 26.7812;
p.tau2 = 198;
p.   n = 3;
p.   A = 0.2453;
p.   C = 110.1562;


% play stimulus through model
MSGdata.time = 1e-3*(1:length(MSGdata.PID));

for i = 1:length(MSGdata.paradigm)
	[MSGdata.aNL_pred(:,i),~,MSGdata.k_D(:,i)] = aNLN(MSGdata.PID(:,i),p);
end

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
clear ax
for i = 1:3
	ax(i) = subplot(1,3,i); hold on
end

% show how k_D changes with the mean stimulus
c = parula(11);
for i = 1:length(MSGdata.paradigm)
	errorbar(ax(1),mean(MSGdata.PID(35e3:55e3,i)),mean(MSGdata.k_D(35e3:55e3,i)),std(MSGdata.k_D(35e3:55e3,i)),'Color',c(MSGdata.paradigm(i),:))
end
xlabel(ax(1),'\mu_{Stimulus} (V)')
ylabel(ax(1),'k_D (V)')

% recover filters for every trial
for i = 1:length(MSGdata.paradigm)
	MSGdata.Khat(:,i) = fitFilter2Data(MSGdata.PID(35e3:55e3,i),MSGdata.aNL_pred(35e3:55e3,i),'reg',1,'offset',100,'filter_length',700);
	MSGdata.Khat(:,i) = MSGdata.Khat(:,i)/norm(MSGdata.Khat(:,i));
	MSGdata.Khat(:,i) = MSGdata.Khat(:,i)*norm(MSGdata.K);
end


% reproject all the stimuli
for i = 1:length(MSGdata.paradigm)
	MSGdata.K_pred(:,i) = mean(MSGdata.PID(35e3:55e3,i)) + convolve(MSGdata.time,MSGdata.PID(:,i),MSGdata.Khat(:,i),MSGdata.filtertime);
end

% make i/o curves for different mean stimuli and compute gain in each paradigm 
axes(ax(2)), hold(ax(2),'on')
for i = 1:max(MSGdata.paradigm)
	x = mean(MSGdata.K_pred(35e3:55e3,MSGdata.paradigm==i),2);
	y = mean(MSGdata.aNL_pred(35e3:55e3,MSGdata.paradigm==i),2);
	plotPieceWiseLinear(x,y,'nbins',50,'Color',c(i,:),'show_error',false,'LineWidth',2);
end
xlabel(ax(2),'Projected Stimulus (V)')
ylabel(ax(2),'Adaptive NLN model prediction (Hz)')

for i = 1:length(MSGdata.paradigm)
	x = MSGdata.K_pred(35e3:55e3,i);
	y = MSGdata.aNL_pred(35e3:55e3,i);
	ff = fit(x(:),y(:),'poly1');
	MSGdata.NL_gain(i) = ff.p1;
end

mean_stim = mean(MSGdata.PID(35e3:55e3,:));
for i = 1:max(MSGdata.paradigm)
	plot(ax(3),mean_stim(MSGdata.paradigm == i), MSGdata.NL_gain(MSGdata.paradigm==i),'+','Color',c(i,:))
end
set(ax(3),'XScale','log','YScale','log','XLim',[.2 2],'YLim',[20 200],'XTick',[.1 .2 .5 1])
xlabel(ax(3),'Mean stimulus (V)')
ylabel(ax(3),'Gain (Hz/V)')

prettyFig('fs',16)


if being_published	
	snapnow	
	delete(gcf)
end

%% Version Info
%
pFooter;


