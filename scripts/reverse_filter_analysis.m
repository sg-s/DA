

pHeader;

%% Reverse Filter analysis
% In this document, I assume NL-type models underlie data that we have, and attempt to a) recover this input nonlinearity non-parametrically and b) check if this input nonlinearity changes with stimulus condition or time. The way I do this is to first extract a filter from the response to the stimulus (a-causally), and then to plot the stimulus vs. projected response. 

%% Validation with synthetic data: naturalistic stimuli
% First, I validate this analysis method using synthetic data from a DA model. In the following figure, I use three different DA models run on the sparse naturalistic stimulus in Fig 2 of the paper:

%%
% 
% # A DA model fit to the naturalistic stimulus data (first row) 
% # The same DA model, but with gain control turned off (by setting B := 0) (second row)
% # The same model as in the first row, but forcing the $K_z$ and $K_y$ filters to be the same (third row). This effective transforms the DA model into a LN model. 

%%
% In each row, the first column shows the parameters (filters) of the DA model I used to generate the synthetic data. The second column shows the reconstructed filters (from the response to the stimulus). Note that in all cases, most of the filter is a-causal (negative time is in the future). In the third column, I'm plotting the convolution of the filter with the response generated by the DA model vs. the stimulus. This is the reconstructed input nonlinearity. I'm colouring the points by the mean stimulus in the preceding 300ms (log-weighted). In (c), estimated stimulus correlates poorly with the stimulus. Is this because of the fast gain control or is it because of the inherent nonlinearity involved in the DA model? Let's see: In (f), where there is no gain control (by construction), the estimated stimulus correlates well with the stimulus. However, in (i), where there is also no gain control, but where I am fitting a NL model to LN model responses, we see apparent movement of the input nonlinearity. 

%%
% (c) looks very similar to (i). I can't tell these two apart. So it's hard to distinguish "true gain control" from a LN model using this approach. (What is not shown is that it is also hard to distinguish true gain control from a NL model by fitting models to the data). 


% get the first nat. stim
load(getPath(dataManager,'5c7dacc5b42ff0eebb980d80fec120c3'),'data','spikes')
PID = data(2).PID;
fA = spiketimes2f(spikes(2).A,1e-4*(1:length(PID)));
fA = mean(fA,2);
PID = PID(:,1:10:end)';
PID = PID - min(min(PID(1:5e3,:)));
S = mean(PID,2);
time = 1e-3*(1:length(S));

% fit a DA model to this
clear p
p.   s0 = 0;
p.  n_z = 2;
p.tau_z = 147.3750;
p.  n_y = 2;
p.tau_y = 27.2500;
p.    C = 0.5000;
p.    A = 170.4375;
p.    B = 2.7656;

[R0,~,~,Ky,Kz] = DAModelv2(S,p);

% now generate another set of responses with the dynamical adaptation turned off
p.B = 0;
R1 = DAModelv2(S,p);

% and another one which is effectively a LN model
p.B = 2.7656;
p.C = 1;
[R2,~,~,~,Kz_2] = DAModelv2(S,p);

% treat t the stimulus and the response as each other, and flip time
K0 = fitFilter2Data(R0,S,'reg',1,'filter_length',1e3,'offset',600);
filtertime = 1e-3*(1:length(K0)) - .6;
S0 = convolve(time,R0,K0,filtertime)/max(R0);

K1 = fitFilter2Data(R1,S,'reg',1,'filter_length',1e3,'offset',600);
S1 = convolve(time,R1,K1,filtertime)/max(R1);

K2 = fitFilter2Data(R2,S,'reg',1,'filter_length',1e3,'offset',600);
S2 = convolve(time,R2,K2,filtertime)/max(R2);

history_length = 300;

figure('outerposition',[0 0 910 902],'PaperUnits','points','PaperSize',[910 902]); hold on
subplot(3,3,1); hold on
plot(Ky*1e3)
plot(Kz*1e3)
legend({'K_y','K_z'})
xlabel('Filter lag (ms)')
ylabel('Filter amplitude (a.u.)')
title('DA Model parameters')

subplot(3,3,2); hold on
plot(filtertime,K0,'k')
xlabel('Lag (s)')
ylabel('Filter amplitude (a.u.)')
title('Reconstructed filters (R \rightarrow S)')
set(gca,'XLim',[-.6 .2])

subplot(3,3,3); hold on
shat = computeSmoothedStimulus(S,history_length);
shat = shat-min(shat);
shat = log(1+shat);
shat = shat/max(shat);
shat = 1 + ceil(shat*99);
shat(isnan(shat)) = 1;
cc = parula(100);
c = cc(shat,:);
scatter(S,S0,20,c,'filled')
set(gca,'XScale','log')
legend(['r^2 = ' oval(rsquare(S,S0))],'Location','northwest')
set(gca,'XTick',[1e-2 1e-1 1 10],'XLim',[1e-3 10])
xlabel('Stimulus (V)')
ylabel('K \otimes R (norm)')
title('"True gain control"')

subplot(3,3,4); hold on
plot(Ky*1e3)
plot(Kz*0)
legend({'K_y','K_z'})
xlabel('Filter lag (ms)')
ylabel('Filter amplitude (a.u.)')
title('DA Model parameters')

subplot(3,3,5); hold on
plot(filtertime,K1,'k')
xlabel('Lag (s)')
ylabel('Filter amplitude (a.u.)')
title('Reconstructed filters (R \rightarrow S)')
set(gca,'XLim',[-.6 .2])

subplot(3,3,6); hold on
scatter(S,S1,20,c,'filled')
set(gca,'XScale','log')
legend(['r^2 = ' oval(rsquare(S,S1))],'Location','northwest')
set(gca,'XTick',[1e-2 1e-1 1 10],'XLim',[1e-3 10])
xlabel('Stimulus (V)')
ylabel('K \otimes R (norm)')
title('"No gain control"')

subplot(3,3,7); hold on
plot(Ky*1e3)
plot(Kz_2*1e3)
legend({'K_y','K_z'})
xlabel('Filter lag (ms)')
ylabel('Filter amplitude (a.u.)')
title('DA Model parameters')

subplot(3,3,8); hold on
plot(filtertime,K2,'k')
xlabel('Lag (s)')
ylabel('Filter amplitude (a.u.)')
title('Reconstructed filters (R \rightarrow S)')
set(gca,'XLim',[-.6 .2])

subplot(3,3,9); hold on
scatter(S,S2,20,c,'filled')
set(gca,'XScale','log')
legend(['r^2 = ' oval(rsquare(S,S2))],'Location','northwest')
set(gca,'XTick',[1e-2 1e-1 1 10],'XLim',[1e-3 10])
xlabel('Stimulus (V)')
ylabel('K \otimes R (norm)')
title('"NL model fit to LN model"')

prettyFig('fs',15)

labelFigure

if being_published	
	snapnow	
	delete(gcf)
end

%% Sparse naturalistic stimuli: ab3A responses
% Now I repeat this analysis, but on the real neuron's data. It looks like the input nonlinearity is changing (b), but based on my previous analysis with synthetic data, this result is consistent both with there being "true" gain control and the existence of an output nonlinearity. 


% treat t the stimulus and the response as each other, and flip time
K0 = fitFilter2Data(fA,S,'reg',1,'filter_length',1e3,'offset',600);
filtertime = 1e-3*(1:length(K0)) - .6;
S0 = convolve(time,fA,K0,filtertime)/max(R0);

figure('outerposition',[0 0 1001 502],'PaperUnits','points','PaperSize',[1001 502]); hold on
subplot(1,2,1); hold on
plot(filtertime,K0,'k')
xlabel('Lag (s)')
ylabel('Filter amplitude (a.u.)')
title('Reconstructed filters (R \rightarrow S)')
set(gca,'XLim',[-.6 .2])

subplot(1,2,2); hold on
scatter(S,S0,20,c,'filled')
title('Reconstructed input NL')
set(gca,'XScale','log')
legend(['r^2 = ' oval(rsquare(S,S0))],'Location','northwest')
set(gca,'XTick',[1e-2 1e-1 1 10],'XLim',[1e-3 10])
xlabel('Stimulus (V)')
ylabel('K \otimes R (norm)')

prettyFig()

labelFigure

if being_published	
	snapnow	
	delete(gcf)
end

%% Validation with synthetic data: mean shifted Gaussians
% In the following figure, I use a DA model fit to ab3A responses to generate synthetic data. The stimulus fed into the DA model is the Gaussian data in Fig 1 in the paper. The filters of the DA model are shown in (a). I then reconstruct filters from the response to the stimulus as before (b). I use that to reconstruct input nonlinearities for each trial, and colour them by mean stimulus (c). We see that the reconstructed input nonlinearity appears to move to the right with increasing mean stimulus. 

% get MSG data
clear MSGdata
MSGdata = consolidateData2(getPath(dataManager,'93ba5d68174e3df9f462a1fc48c581da'));
MSGdata = cleanMSGdata(MSGdata);

% get the DA model fit for this
clear p
p.   s0 = -0.1503;
p.  n_z = 2;
p.tau_z = 255.3750;
p.  n_y = 2;
p.tau_y = 20.0748;
p.    C = 0.2345;
p.    A = 2.8700e+03;
p.    B = 100;

% generate responses using the DA model
MSGdata.DA_R = NaN*MSGdata.fA;
for i = 1:length(MSGdata.paradigm)
	[MSGdata.DA_R(:,i),~,~,Ky,Kz] = DAModelv2(MSGdata.PID(:,i),p);
end

% extract reverse filters for each trial 
MSGdata.K = NaN(800,length(MSGdata.paradigm));
MSGdata.Shat = NaN*MSGdata.fA;
MSGdata.filtertime = 1e-3*(1:length(MSGdata.K)) - .5;
time = 1e-3*(1:length(MSGdata.PID));
for i = 1:length(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,i);
	R = MSGdata.DA_R(35e3:55e3,i);
	K = fitFilter2Data(R,S,'reg',1,'filter_length',1e3,'offset',600);
	K = K(100:end-101); K = K/norm(K);
	MSGdata.Shat(:,i) = convolve(time,MSGdata.DA_R(:,i),K,MSGdata.filtertime);
	MSGdata.K(:,i) = K;
end

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
subplot(1,3,1); hold on
plot(Ky*1e3)
plot(Kz*1e3)
legend({'K_y','K_z'})
xlabel('Filter lag (ms)')
ylabel('Filter amplitude (a.u.)')
title('DA Model parameters')

subplot(1,3,2); hold on
c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	plot(MSGdata.filtertime,MSGdata.K(:,MSGdata.paradigm == i),'Color',c(i,:))
end
xlabel('Filter lag (s)')
ylabel('Filter amplitude (norm)')
title('Reconstructed filters')

ax = subplot(1,3,3); hold on
c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,MSGdata.paradigm == i);
	if i == 1
		S(:,5) = NaN; % outlier
	end
	Shat = MSGdata.Shat(35e3:55e3,MSGdata.paradigm == i);
	plotPieceWiseLinear(S(:),Shat(:),'Color',c(i,:),'nbins',30);
end
ax.YLim(1) = 0;
ax.XLim(1) = 0;
xlabel('Stimulus (V)')
ylabel('K \otimes R')

prettyFig();

labelFigure

if being_published
	snapnow
	delete(gcf)
end

%%
% Now, what if we repeat this analysis, but use a DA model with gain controlled turned off? Once again, I do this by setting $K_y = K_z$, so that the DA model effectively becomes a LN model. 

p.s0 = 0;
p.C = 1;

% generate responses using the DA model
MSGdata.DA_R = NaN*MSGdata.fA;
for i = 1:length(MSGdata.paradigm)
	[MSGdata.DA_R(:,i),~,~,Ky,Kz] = DAModelv2(MSGdata.PID(:,i),p);
end

% extract reverse filters for each trial 
MSGdata.K = NaN(800,length(MSGdata.paradigm));
MSGdata.Shat = NaN*MSGdata.fA;
MSGdata.filtertime = 1e-3*(1:length(MSGdata.K)) - .5;
time = 1e-3*(1:length(MSGdata.PID));
for i = 1:length(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,i);
	R = MSGdata.DA_R(35e3:55e3,i);
	K = fitFilter2Data(R,S,'reg',1,'filter_length',1e3,'offset',600);
	K = K(100:end-101); K = K/norm(K);
	MSGdata.Shat(:,i) = convolve(time,MSGdata.DA_R(:,i),K,MSGdata.filtertime);
	MSGdata.K(:,i) = K;
end

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
subplot(1,3,1); hold on
plot(Ky*1e3)
plot(Kz*1e3)
legend({'K_y','K_z'})
xlabel('Filter lag (ms)')
ylabel('Filter amplitude (a.u.)')
title('DA Model parameters')

subplot(1,3,2); hold on
c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	plot(MSGdata.filtertime,MSGdata.K(:,MSGdata.paradigm == i),'Color',c(i,:))
end
xlabel('Filter lag (s)')
ylabel('Filter amplitude (norm)')
title('Reconstructed filters')

ax = subplot(1,3,3); hold on
c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,MSGdata.paradigm == i);
	if i == 1
		S(:,5) = NaN; % outlier
	end
	Shat = MSGdata.Shat(35e3:55e3,MSGdata.paradigm == i);
	plotPieceWiseLinear(S(:),Shat(:),'Color',c(i,:),'nbins',30);
end
ax.YLim(1) = 0;
ax.XLim(1) = 0;
xlabel('Stimulus (V)')
ylabel('K \otimes R')

prettyFig();

labelFigure

if being_published
	snapnow
	delete(gcf)
end

%%
% Now, I repeat this analysis on the real data. 

% extract reverse filters for each trial 
MSGdata.K = NaN(800,length(MSGdata.paradigm));
MSGdata.Shat = NaN*MSGdata.fA;
MSGdata.filtertime = 1e-3*(1:length(MSGdata.K)) - .5;
time = 1e-3*(1:length(MSGdata.PID));
for i = 1:length(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,i);
	R = MSGdata.fA(35e3:55e3,i);
	try
		K = fitFilter2Data(R,S,'reg',1,'filter_length',1e3,'offset',600);
		K = K(100:end-101); K = K/norm(K);
		MSGdata.Shat(:,i) = convolve(time,MSGdata.fA(:,i),K,MSGdata.filtertime);
		MSGdata.K(:,i) = K;
	catch
	end
end

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
subplot(1,3,1); hold on
plot(Ky*1e3)
plot(Kz*1e3)
legend({'K_y','K_z'})
xlabel('Filter lag (ms)')
ylabel('Filter amplitude (a.u.)')
title('DA Model parameters')

subplot(1,3,2); hold on
c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	plot(MSGdata.filtertime,MSGdata.K(:,MSGdata.paradigm == i),'Color',c(i,:))
end
xlabel('Filter lag (s)')
ylabel('Filter amplitude (norm)')
title('Reconstructed filters')

ax = subplot(1,3,3); hold on
c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,MSGdata.paradigm == i);
	if i == 1
		S(:,5) = NaN; % outlier
	end
	Shat = MSGdata.Shat(35e3:55e3,MSGdata.paradigm == i);
	plotPieceWiseLinear(S(:),Shat(:),'Color',c(i,:),'nbins',30);
end
ax.YLim(1) = 0;
ax.XLim(1) = 0;
xlabel('Stimulus (V)')
ylabel('K \otimes R')

prettyFig();

labelFigure

if being_published
	snapnow
	delete(gcf)
end

%%
% Unsurprisingly, filter estimation is hard as the mean stimulus increases. But I still see the basic effect: input nonlinearities still move to the right, meaning that this is "true gain control". 

%%
% What if I only use filters estimated from the lowest doses (where the responses and signal are cleanest)? Does that then help me estimate the input nonlinearities more cleanly? 

% extract reverse filters for each trial 
MSGdata.Shat = NaN*MSGdata.fA;

% use K only from the lowest paradigm
K = mean(MSGdata.K(:,MSGdata.paradigm==1),2);
for i = 1:length(MSGdata.paradigm)
	MSGdata.Shat(:,i) = convolve(time,MSGdata.fA(:,i),K,MSGdata.filtertime);
end

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1001 500]); hold on

c = parula(max(MSGdata.paradigm)+1);
for i = 1:max(MSGdata.paradigm)
	S = MSGdata.PID(35e3:55e3,MSGdata.paradigm == i);
	if i == 1
		S(:,5) = NaN; % outlier
	end
	Shat = MSGdata.Shat(35e3:55e3,MSGdata.paradigm == i);
	plotPieceWiseLinear(S(:),Shat(:),'Color',c(i,:),'nbins',30);
end
xlabel('Stimulus (V)')
ylabel('K \otimes R')
title('Using a single filter')

prettyFig();


if being_published
	snapnow
	delete(gcf)
end

%% Version Info
%
pFooter;


