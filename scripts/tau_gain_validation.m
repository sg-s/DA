
pHeader;




%% Validation of estimation of gain control timescale
% In this document, I attempt to validate methods by which I estimate the timescale of gain control from naturalistic stimuli. 

   ;;;    ;;;;;;;;   ;;;;;;;     ;;;    
  ;; ;;   ;;     ;; ;;     ;;   ;; ;;   
 ;;   ;;  ;;     ;;        ;;  ;;   ;;  
;;     ;; ;;;;;;;;   ;;;;;;;  ;;     ;; 
;;;;;;;;; ;;     ;; ;;        ;;;;;;;;; 
;;     ;; ;;     ;; ;;        ;;     ;; 
;;     ;; ;;;;;;;;  ;;;;;;;;; ;;     ;; 

 ;;;;;;;          ;;;;;;;;  ;;     ;; ;;;;;;;;    ;;;    ;;    ;;  ;;;;;;;  ;;    ;; ;;;;;;;; 
;;     ;;         ;;     ;; ;;     ;;    ;;      ;; ;;   ;;;   ;; ;;     ;; ;;;   ;; ;;       
       ;;         ;;     ;; ;;     ;;    ;;     ;;   ;;  ;;;;  ;; ;;     ;; ;;;;  ;; ;;       
 ;;;;;;;  ;;;;;;; ;;;;;;;;  ;;     ;;    ;;    ;;     ;; ;; ;; ;; ;;     ;; ;; ;; ;; ;;;;;;   
;;                ;;     ;; ;;     ;;    ;;    ;;;;;;;;; ;;  ;;;; ;;     ;; ;;  ;;;; ;;       
;;                ;;     ;; ;;     ;;    ;;    ;;     ;; ;;   ;;; ;;     ;; ;;   ;;; ;;       
;;;;;;;;;         ;;;;;;;;   ;;;;;;;     ;;    ;;     ;; ;;    ;;  ;;;;;;;  ;;    ;; ;;;;;;;; 




%% ab2A and 2-butanone
% First, does this method yield somethign when applied to real data?  Can we see if there is any gain control in this data? In the following figure, I first fit NLN-models neuron-by-neuron to the data. The NLN models have only two parameters (the $k_D$ and the $n$) that are fit parametrically. Other parameters (like the filter) are fit non-parametrically. In the following figure, I plot the results of each neuron in a separate colour. The first plot shows the distribution of the deviations of the NLN model predictions from the measured response. The second plot shows the Spearman correlation between the deviations and the mean stimulus in some preceding window, as a function of window length. Note that these plots tend to have a minimum at some defined timescale. The dotted and dashed lines indicate the autocorrelation times for the stimulus and the response respectively. 


% get all data 
cdata = consolidateData2(getPath(dataManager,'4608c42b12191b383c84fea52392ea97'));
[~, data] =  assembleScaledNatStim(cdata);


clear p

p(1).k_D = .6578;
p(1).n = .7188;

p(2).  k_D = 0.1109;
p(2).    n = .9688;

p(3).k_D = .1187;
p(3).n = .7812;

% generate responses using this model 
warning off
for i = 1:length(data)
	for j = 1:size(data(i).X,2)
		try
			data(i).P(:,j) = NLNmodel([data(i).S(:,j) - min(data(i).S(:,j)) data(i).R(:,j)] ,p(i));
		catch
		end
	end
end
warning on

figure('outerposition',[0 0 1220 601],'PaperUnits','points','PaperSize',[1220 601]); hold on
clear ax
ax(2) = subplot(1,2,1); hold on
ax(4) = subplot(1,2,2); hold on

c = lines(3);

for i = 1:3
	plot_tau_gain_nat_stim(data(i),ax,'c',c(i,:),'response_cutoff',30);
end

prettyFig();
suptitle('ab2A -- 2-butanone')

if being_published
	snapnow
	delete(gcf)
end

%%
% It looks like there is gain control that is not being captured by the NLN model on a timescale of a few seconds. Is this actually true? I need to validate this analysis method to ensure that what we see isn't the result of fitting a model to data that is actually generated by a qualitatively different model. 

%%
% The first order of business is to check if we really need to use Spearman correlations. Can we use simple Pearson correlations? In the following figure, I repeat the same analysis, but using Pearson correlations. 

figure('outerposition',[0 0 1220 601],'PaperUnits','points','PaperSize',[1220 601]); hold on
clear ax
ax(2) = subplot(1,2,1); hold on
ax(4) = subplot(1,2,2); hold on

c = lines(3);

for i = 1:3
	plot_tau_gain_nat_stim(data(i),ax,'c',c(i,:),'method','Pearson');
end

prettyFig();
suptitle('ab2A -- 2-butanone')

if being_published
	snapnow
	delete(gcf)
end


%%
% This doesn't look so different, so that's good. 

%%
% Now, I plot the residuals as a function of the response to see if there is any structure there. 

figure('outerposition',[0 0 1501 500],'PaperUnits','points','PaperSize',[1501 500]); hold on

for i = 1:length(data)
	subplot(1,3,i); hold on
	D = data(i).R(:) - data(i).P(:);
	x = data(i).R(:);
	l = plot(x(1:10:end),D(1:10:end),'k.');
	legend(l,['r^2 = ' oval(rsquare(x,D))],'Location','southeast')
	set(gca,'YLim',[-80 60])
	xlabel('Response (Hz)')
	ylabel('Deviation from NLN model (Hz)')
end

prettyFig();

if being_published
	snapnow
	delete(gcf)
end


%%
% There seems to be some structure at low firing rates. But since I cut that off anyway in the analysis, it shouldn't matter. 

 ;;;;;;  ;;     ;; ;;;;;;;;  ;;;;;;;  ;;;;;;;; ;;;;;;;;  ;;;;;;  
;;    ;; ;;     ;;    ;;    ;;     ;; ;;       ;;       ;;    ;; 
;;       ;;     ;;    ;;    ;;     ;; ;;       ;;       ;;       
;;       ;;     ;;    ;;    ;;     ;; ;;;;;;   ;;;;;;    ;;;;;;  
;;       ;;     ;;    ;;    ;;     ;; ;;       ;;             ;; 
;;    ;; ;;     ;;    ;;    ;;     ;; ;;       ;;       ;;    ;; 
 ;;;;;;   ;;;;;;;     ;;     ;;;;;;;  ;;       ;;        ;;;;;;  


%% Validation: response cutoff
% In this analysis, I only analyse the responses to whiffs, and neglect deviations from model fit when there is no stimulus. The reason is because when there is no stimulus, the baseline firing of the neuron deviates randomly from the model responses, which are not very meaningful. However, does this cutoff affect this analysis? To determine this, I generate synthetic responses using a NLN model, and then fit a NLN model back to this, and repeat this analysis with different cutoffs. 

% get the real filter
[~,K] = NLNmodel([data(3).S(:,2) - min(data(3).S(:,2)) data(3).R(:,2)] ,p(3));
filtertime = (1:length(K)) - 51;

clear q
q.n = .7812;
q.k_D = .1187;

% generate synthetic data
clear syn_data
S = data(3).S(:,2);
time = 1:length(S);
a = 1./(1+(q.k_D./S).^q.n);
R = convolve(time,a,K,filtertime);
R(R<0) = 0;
R = R*100;
syn_data.S = S;
syn_data.R = R;
% make predictions
syn_data.P = NLNmodel([S R],q);

figure('outerposition',[0 0 1501 555],'PaperUnits','points','PaperSize',[1501 555]); hold on
clear ax
ax(2) = subplot(1,3,1); hold on; axis square
ax(4) = subplot(1,3,2); hold on; axis square
clear ax2
ax2(4) = subplot(1,3,3); hold on; axis square

all_cutoffs = [0 5 10 20];
c = parula(length(all_cutoffs)+1);
for i = 1:length(all_cutoffs)
	plot_tau_gain_nat_stim(syn_data,ax,'c',c(i,:),'response_cutoff',all_cutoffs(i),'method','Spearman');
	plot_tau_gain_nat_stim(syn_data,ax2,'c',c(i,:),'response_cutoff',all_cutoffs(i),'method','Pearson');
end
suptitle('synthetic data: effect of response cutoff ')


set(ax(4),'YLim',[-1 1])
set(ax2(4),'YLim',[-1 1])

l = flipud(ax(2).Children); clear L
for i = 1:length(l)
	L{i} = ['R>' oval(all_cutoffs(i))];
end
legend(l,L);
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%%
% Varying the response cutoff seems to do all sorts of stuff to these plots. Most importantly, it varies the instantaneous correlation. So perhaps we should set the response cutoff so that the instantaneous correlation is zero? That's one thought. While it is troubling that there seems to be some structure in the correlation vs. timescale plots, note that the deviations from the model are tiny, and all these correlations arise from tiny mismatches. 

;;     ;; ;;    ;;  ;;;;;;   ;;;;;;     ;;;    ;;       ;;;;;;;; ;;;;;;;;  
;;     ;; ;;;   ;; ;;    ;; ;;    ;;   ;; ;;   ;;       ;;       ;;     ;; 
;;     ;; ;;;;  ;; ;;       ;;        ;;   ;;  ;;       ;;       ;;     ;; 
;;     ;; ;; ;; ;;  ;;;;;;  ;;       ;;     ;; ;;       ;;;;;;   ;;     ;; 
;;     ;; ;;  ;;;;       ;; ;;       ;;;;;;;;; ;;       ;;       ;;     ;; 
;;     ;; ;;   ;;; ;;    ;; ;;    ;; ;;     ;; ;;       ;;       ;;     ;; 
 ;;;;;;;  ;;    ;;  ;;;;;;   ;;;;;;  ;;     ;; ;;;;;;;; ;;;;;;;; ;;;;;;;;  

;;;;;;;;  ;;;;;;;;    ;;;    ;;;;;;;;   ;;;;;;   ;;;;;;;  ;;    ;; 
;;     ;; ;;         ;; ;;   ;;     ;; ;;    ;; ;;     ;; ;;;   ;; 
;;     ;; ;;        ;;   ;;  ;;     ;; ;;       ;;     ;; ;;;;  ;; 
;;;;;;;;  ;;;;;;   ;;     ;; ;;;;;;;;   ;;;;;;  ;;     ;; ;; ;; ;; 
;;        ;;       ;;;;;;;;; ;;   ;;         ;; ;;     ;; ;;  ;;;; 
;;        ;;       ;;     ;; ;;    ;;  ;;    ;; ;;     ;; ;;   ;;; 
;;        ;;;;;;;; ;;     ;; ;;     ;;  ;;;;;;   ;;;;;;;  ;;    ;; 


%%
% Perhaps a way to combine the effect of the correlation of the deviations and the scale of the deviations would be to use the unscaled Pearson correlation coefficient 
%
% $$ P_{unscaled}(x,y)=\frac{{\displaystyle \sum_{i}(x_{i}-\bar{x})(y_{i}-\bar{y})}}{\sqrt{\sum_{i}(x_{i}-\bar{x})^{2}}} $$
% 
% which has the units of Hz, where $y$ is the deviations and $x$ is the mean stimulus in preceding window. 


figure('outerposition',[0 0 1112 901],'PaperUnits','points','PaperSize',[1112 901]); hold on

clear ax
ax(2) = subplot(2,3,1); hold on; axis square
ax(3) = subplot(2,3,2); hold on; axis square
ax(4) = subplot(2,3,3); hold on; axis square


all_cutoffs = [0 5 10 20];
c = parula(length(all_cutoffs)+1);

for i = 1:length(all_cutoffs)
	plot_tau_gain_nat_stim(data(3),ax,'c',c(i,:),'example_history_length',1e3,'method','unscaled_Pearson','response_cutoff',all_cutoffs(i));
end

set(ax(4),'YLim',[-15 10])
set(ax(3),'YLim',[-30 30],'XLim',[1e-4 1])
title(ax(4),'Real data')
title(ax(2),'Real data')

% now do the synthetic data

clear ax
ax(2) = subplot(2,3,4); hold on; axis square
ax(3) = subplot(2,3,5); hold on; axis square
ax(4) = subplot(2,3,6); hold on; axis square

for i = 1:length(all_cutoffs)
	plot_tau_gain_nat_stim(syn_data,ax,'c',c(i,:),'example_history_length',1e3,'response_cutoff',all_cutoffs(i),'method','unscaled_Pearson');
end

set(ax(4),'YLim',[-15 10])
set(ax(2),'XLim',[-40 40])
set(ax(3),'YLim',[-30 30],'XLim',[1e-4 1])
title(ax(4),'synthetic data')
title(ax(2),'synthetic data')

l = flipud(ax(2).Children); clear L
for i = 1:length(l)
	L{i} = ['R>' oval(all_cutoffs(i))];
end
legend(l,L);
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%%
% As suspected, the synthetic data shows much smaller apparent gain changes compared to the real data. So that's something to keep in mind -- not only do the absolute values of the correlation matter, but also the scale of the deviations. I'm going to use this unscaled Pearson correlation going forward. 


;;;; ;;    ;; ;;;;;;;;  ;;     ;; ;;;;;;;;    ;;    ;; ;;       
 ;;  ;;;   ;; ;;     ;; ;;     ;;    ;;       ;;;   ;; ;;       
 ;;  ;;;;  ;; ;;     ;; ;;     ;;    ;;       ;;;;  ;; ;;       
 ;;  ;; ;; ;; ;;;;;;;;  ;;     ;;    ;;       ;; ;; ;; ;;       
 ;;  ;;  ;;;; ;;        ;;     ;;    ;;       ;;  ;;;; ;;       
 ;;  ;;   ;;; ;;        ;;     ;;    ;;       ;;   ;;; ;;       
;;;; ;;    ;; ;;         ;;;;;;;     ;;       ;;    ;; ;;;;;;;; 


%% Validation: input nonlinearity 
% In this section, I generate synthetic data using a model where the input nonlinearity does not go to 0 or 1 at its extremities. The NLN model I then fit to this synthetic data cannot capture this input nonlinearity, and is forced to approximate it using a "wrong" functional form. 


all_min_a = [0 .1 .2 .3];
all_max_a = [1 .9 .8 .5];

clear q
q.n = .7812;
q.k_D = .1187;

% generate synthetic data
clear syn_data
for i = 1:length(all_max_a)
	S = data(3).S(:,2);
	a = 1./(1+(q.k_D./S).^q.n);
	a = a*(all_max_a(i) - all_min_a(i)) + all_min_a(i);
	time = 1:length(S);
	R = convolve(time,a,K,filtertime);
	R(R<0) = 0;
	R = R*100;
	syn_data(i).S = S;
	syn_data(i).R = R;
	% make predictions
	syn_data(i).P = NLNmodel([S R],q);
end


c = lines(length(syn_data));

figure('outerposition',[0 0 901 902],'PaperUnits','points','PaperSize',[901 902]); hold on
subplot(2,2,1); hold on
for i = 1:length(all_min_a)
	x = logspace(-4,2,100);
	a = 1./(1+(q.k_D./x).^q.n);
	a = a*(all_max_a(i) - all_min_a(i)) + all_min_a(i);
	plot(x,a,'Color',c(i,:))
end
xlabel('Stimulus (V)')
ylabel('a')
title('Synthetic input nonlinearities')
set(gca,'XScale','log')
axis square

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square


for i = 1:length(syn_data)
	rc = findResponseCutoffTauGainPlot(syn_data(i).S,syn_data(i).R,syn_data(i).P,'unscaled_Pearson');
	plot_tau_gain_nat_stim(syn_data(i),ax,'c',c(i,:),'response_cutoff',rc,'example_history_length',1e3,'method','unscaled_Pearson');
end
suptitle('synthetic data: effect of input nonlinearity mismatch')
set(ax(4),'YLim',[-10 10])
set(ax(3),'YLim',[-10 10])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

;;        ;;;;;;;   ;;;;;;      ;;    ;; ;;       
;;       ;;     ;; ;;    ;;     ;;;   ;; ;;       
;;       ;;     ;; ;;           ;;;;  ;; ;;       
;;       ;;     ;; ;;   ;;;;    ;; ;; ;; ;;       
;;       ;;     ;; ;;    ;;     ;;  ;;;; ;;       
;;       ;;     ;; ;;    ;;     ;;   ;;; ;;       
;;;;;;;;  ;;;;;;;   ;;;;;;      ;;    ;; ;;;;;;;; 


%%
% Now I choose a qualitatively different input nonlinearity -- a log function, and repeat the analysis to see if the timescale we saw in the data pops back up. 

a = log(S);
a = a - min(a); a = a/max(a);
R = convolve(time,a,K,filtertime);
R(R<0) = 0;
R = R*100;

clear syn_data
syn_data.S = S;
syn_data.R = R;

% find best fit model params
clear p
p.n = 0.53;
p.k_D = .0313;

% make predictions
syn_data.P = NLNmodel([S R],p);


figure('outerposition',[0 0 902 901],'PaperUnits','points','PaperSize',[902 901]); hold on
subplot(2,2,1); hold on
x = logspace(-4,2,100);
a = log(x);
a = a - min(a); a = a/max(a);
plot(x,a,'k');
a = 1./(1 + (p.k_D./x).^p.n);
plot(x,a,'r')

xlabel('Stimulus (V)')
ylabel('a')
title('Synthetic input nonlinearities: a log')
set(gca,'XScale','log')
axis square

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square

rc = findResponseCutoffTauGainPlot(syn_data.S,syn_data.R,syn_data.P,'unscaled_Pearson');
plot_tau_gain_nat_stim(syn_data,ax,'c',[0 0 0],'response_cutoff',rc,'method','unscaled_Pearson');

suptitle('synthetic data')
set(ax(4),'YLim',[-10 10])
set(ax(3),'YLim',[-10 10])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end
    

 ;;;;;;  ;;;;;;;; ;;;; ;;     ;;         ;;;;;;   ;;;;;;;  ;;;;;;;;  ;;;;;;;;  
;;    ;;    ;;     ;;  ;;;   ;;;        ;;    ;; ;;     ;; ;;     ;; ;;     ;; 
;;          ;;     ;;  ;;;; ;;;;        ;;       ;;     ;; ;;     ;; ;;     ;; 
 ;;;;;;     ;;     ;;  ;; ;;; ;;        ;;       ;;     ;; ;;;;;;;;  ;;;;;;;;  
      ;;    ;;     ;;  ;;     ;;        ;;       ;;     ;; ;;   ;;   ;;   ;;   
;;    ;;    ;;     ;;  ;;     ;;        ;;    ;; ;;     ;; ;;    ;;  ;;    ;;  
 ;;;;;;     ;;    ;;;; ;;     ;;         ;;;;;;   ;;;;;;;  ;;     ;; ;;     ;; 


%% Validation: effect of stimulus correlation time
% Do correlations in th stimulus manifest in weird ways and cause us to think that there is gain control when there isn't? To check this, I rescale the stimulus in the time axis and re-do the analysis to see how stimulus correlations can screw with this method. 

% generate data

clear p
p.k_D = .1187;
p.n = .7812;

stim_scale = [.2 .5 1 2 5];

clear syn_data
for i = 1:length(stim_scale)
	fake_time = linspace(0,max(time),stim_scale(i)*length(time));
	Si  = vectorise(interp1(time,S,fake_time));

	% generate responses using real model
	a = 1./(1 + (p.k_D./Si).^p.n);
	R = convolve(1:length(Si),a,K,filtertime);
	R(R<0) = 0;
	R = R*100;

	% remove nans
	rm_this = isnan(R) | isnan(Si);

	syn_data(i).R = R(~rm_this);
	syn_data(i).S = Si(~rm_this);

	% make prediction
	syn_data(i).P = NLNmodel([ Si(~rm_this) R(~rm_this)],p);
end

c = lines(length(syn_data));

figure('outerposition',[0 0 903 901],'PaperUnits','points','PaperSize',[903 901]); hold on
subplot(2,2,1); hold on
for i = 1:length(syn_data)
	[a,lags] = autocorr(syn_data(i).S,length(syn_data(i).S)-1);
	plot(lags,a,'Color',c(i,:))
end
set(gca,'XScale','log')
xlabel('lag (ms)')
ylabel('Autocorrelation')

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square


for i = 1:length(syn_data)
	rc = findResponseCutoffTauGainPlot(syn_data(i).S,syn_data(i).R,syn_data(i).P,'unscaled_Pearson');
	plot_tau_gain_nat_stim(syn_data(i),ax,'c',c(i,:),'response_cutoff',rc,'method','unscaled_Pearson');
end
suptitle('synthetic data: effect of changing stimulus correlations')
set(ax(4),'YLim',[-10 10])
set(ax(3),'YLim',[-10 10])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

   ;;;    ;;;;;;;;     ;;;    ;;;;;;;;  ;;;;;;;; ;;;; ;;    ;;  ;;;;;;   
  ;; ;;   ;;     ;;   ;; ;;   ;;     ;;    ;;     ;;  ;;;   ;; ;;    ;;  
 ;;   ;;  ;;     ;;  ;;   ;;  ;;     ;;    ;;     ;;  ;;;;  ;; ;;        
;;     ;; ;;     ;; ;;     ;; ;;;;;;;;     ;;     ;;  ;; ;; ;; ;;   ;;;; 
;;;;;;;;; ;;     ;; ;;;;;;;;; ;;           ;;     ;;  ;;  ;;;; ;;    ;;  
;;     ;; ;;     ;; ;;     ;; ;;           ;;     ;;  ;;   ;;; ;;    ;;  
;;     ;; ;;;;;;;;  ;;     ;; ;;           ;;    ;;;; ;;    ;;  ;;;;;;   

;;     ;;  ;;;;;;;  ;;;;;;;;  ;;;;;;;; ;;       
;;;   ;;; ;;     ;; ;;     ;; ;;       ;;       
;;;; ;;;; ;;     ;; ;;     ;; ;;       ;;       
;; ;;; ;; ;;     ;; ;;     ;; ;;;;;;   ;;       
;;     ;; ;;     ;; ;;     ;; ;;       ;;       
;;     ;; ;;     ;; ;;     ;; ;;       ;;       
;;     ;;  ;;;;;;;  ;;;;;;;;  ;;;;;;;; ;;;;;;;; 


%% Validation: adapting NLN model
% In this section, I use an adapting NLN model and attempt to recover the timescale of gain control using this analysis. In the following figure, the first panel shows the autocorrelation function of the dynamically updating $k_D$ in the model. Since it's hard to define the timescale of gain control directly from the parameters of the model, I define the timescale of gain control as the autocorrelation time of the time series of $k_D$. This is indicated by the vertical red line. Note that the unscaled Pearson correlation has a large absolute value (indicating significant deviations from the best-fit model) and that the minimum seems to be roughly in agreement with the red line (the actual gain control timescale). 

clear p
p.   k0 = 0.0451;
p.tau_z = 2424;
p.    B = 0.6250;
p.  n_z = 0.1016;
p.    n = 0.9812;
p. tau1 = 41.2812;
p. tau2 = 45.7812;
p.  n_y = 1.3984;
p.    A = 0.9340;
p.    C = 2.7945e+03;

[R,~,~,k_D] = aNLN2(S,p);

clear syn_data
syn_data.R = R;
syn_data.S = S;

clear p
p.k_D = 0.7589;
p.n = 0.9000;

syn_data.P = NLNmodel([S R],p);

figure('outerposition',[0 0 903 901],'PaperUnits','points','PaperSize',[903 901]); hold on
subplot(2,2,1); hold on

[a,lags] = autocorr(k_D,length(k_D)-1);
plot(lags,a,'Color','k')

act = autoCorrelationTime(k_D);
plot([act act],[-1 1],'r','LineWidth',3)

set(gca,'XScale','log','XTick',[1 10 100 1e3 1e4 1e5],'YLim',[-.2 1])
xlabel('lag (ms)')
ylabel('Autocorrelation')
title('Autocorrelation of k_D')



clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square

plot_tau_gain_nat_stim(syn_data,ax,'c',[0 0 0],'response_cutoff',10,'example_history_length',2e3,'method','unscaled_Pearson');

suptitle('synthetic data: adapting NLN model fit with NLN model')
set(ax(4),'YLim',[-30 10],'XTick',[1 10 100 1e3 1e4 1e5],'XLim',[1 1e5])
set(ax(3),'YLim',[-50 60],'XLim',[1e-5 1e0],'XTick',[1e-5 1e-4 1e-3 1e-2 1e-1 1e0])
plot(ax(4),[act act],[-50 1e3],'r','LineWidth',3)
prettyFig();

if being_published
	snapnow
	delete(gcf)
end


%%
% So that looks good. In this section, I vary the timescale of gain control in the adapting model, and repeat this analysis several times so I can estimate how well I can recover the gain control timescale. 

clear p
p.   k0 = 0.0451;
p.tau_z = 2424;
p.    B = 0.6250;
p.  n_z = 0.1016;
p.    n = 0.9812;
p. tau1 = 41.2812;
p. tau2 = 45.7812;
p.  n_y = 1.3984;
p.    A = 0.9340;
p.    C = 2.7945e+03;



all_tau_z = linspace(300,6e3,10);
actual_tau = NaN*all_tau_z;

load('.cache/NLN_fits_to_aNLN.mat','q')

clear syn_data 
for i = 1:length(all_tau_z)
	p.tau_z = all_tau_z(i);
	[R,~,~,k_D] = aNLN2(S,p);
	syn_data(i).S = S;
	syn_data(i).R = R;
	actual_tau(i) = autoCorrelationTime(k_D);

	% generate responses
	syn_data(i).P = NLNmodel([S R],q(i));
end

figure('outerposition',[0 0 500 500],'PaperUnits','points','PaperSize',[1501 500]); hold on
clear ax

estimated_tau = NaN*actual_tau;

c = parula(length(all_tau_z)+1);
for i = 2:length(syn_data)
	[rho,all_history_lengths] = plot_tau_gain_nat_stim(syn_data(i),NaN(4,1),'c',c(i,:),'response_cutoff',10,'example_history_length',2e3,'method','unscaled_Pearson','all_history_lengths',round(linspace(min(actual_tau)/2,2*max(actual_tau),30)));
	[~,idx] = min(rho);
	estimated_tau(i) = all_history_lengths(idx);

end

l = plot(actual_tau,estimated_tau,'k+');
set(gca,'XLim',[1e3 5e3],'YLim',[1e3 5e3])
plot([0 5e3],[0 5e3],'k--')
r2 = rsquare(actual_tau(2:end),estimated_tau(2:end));
legend(l,['r^2 = ' oval(r2)],'Location','northwest')
xlabel('Autocorrelation time of k_D (ms)')
ylabel('Estimated timescale of gain control (ms)')

prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%% Model mismatch
% There is something very fishy in all analysis. I only see a peak in the correlation vs. timescale plots when the NLN model fits the data poorly. When it does a good job, I see nothing. Is there ever a case where the NLN model fits the data poorly, but the plot of correlation vs. timescale is flat? That would be convincing. 

%%
% To see this, I fit a DA model to the real data, and use that to generate synthetic data. I then treat that as data and repeat the analysis as before. In the following figure, (a) shows the DA model fit to the real data. The fit is quite good. (b) compares the DA model prediction to a NLN model prediction fit to the DA model. Paradoxically, the fit is quite good -- suggesting that the DA model is using its gain control to mimic the input nonlinearity (or the other way round). (c) shows the autocorrelation functions of the stimulus (black) and $y(t)$ in the DA model. Note that there are weird peaks at $~1s$. (d) Here, I reproduce the analysis of the real data for comparison's sake. (e) Analysis of the DA model responses. This suggests that there is some gain control, and the minimum is close to the actual gain control timescale in the DA model (red line). What is worrying, though is that the minimum in both the data and the synthetic data seems to be at 1s -- which is where there is a weird peak in the stimulus autocorrelation function. I need to go back to this data and check that the minimum in these plots isn't ALWAYS correlated with peaks in the stimulus autocorrelation function. 

clear p_DA
p_DA.   s0 = -4.8828e-04;
p_DA.  n_z = 7.1875;
p_DA.tau_z = 39.3750;
p_DA.  n_y = 8.5469;
p_DA.tau_y = 5.9531;
p_DA.    C = 0.4992;
p_DA.    A = 3.2929e+03;
p_DA.    B = 23.5625;

clear syn_data
syn_data.S =  S;
[syn_data.R,~,z] = DAModelv2(S,p_DA);

clear p
p.k_D = 0.1063;
p.  n = 0.8000;

% generate NLN predictions
syn_data.P = NLNmodel([syn_data.S syn_data.R],p);

figure('outerposition',[0 0 1300 901],'PaperUnits','points','PaperSize',[1300 901]); hold on

% plot DA model vs actual response
subplot(2,3,1); hold on
plot(syn_data.R,data(3).R(:,2),'k.')
xlabel('DA model (Hz)')
ylabel('ab2A response (Hz)')
legend(['r^2=' oval(rsquare(syn_data.R,data(3).R(:,2)))],'Location','southeast')

% show NLN model vs. DA model 
subplot(2,3,2); hold on
plot(syn_data.P,syn_data.R,'k.')
ylabel('DA model (Hz)')
xlabel('NLN model prediction (Hz)')
legend(['r^2=' oval(rsquare(syn_data.R,syn_data.P))],'Location','southeast')

% shot the correlations 
subplot(2,3,3); hold on
[a,lags] = autocorr(z,6e4);
plot(lags,a,'r')

[a,lags] = autocorr(S,6e4);
plot(lags,a,'k')
legend({'DA model y(t)','Stimulus'})
set(gca,'XScale','log','XTick',[1 10 100 1e3 1e4 1e5])
xlabel('Lag (ms)')
ylabel('Autocorrelation')

% do the analysis on the real data again
clear ax
ax(4) = subplot(2,3,4); hold on; axis square
plot_tau_gain_nat_stim(data(3),ax,'response_cutoff',30,'example_history_length',2e3,'method','unscaled_Pearson');
set(ax(4),'YLim',[-15 10],'XTick',[1 10 100 1e3 1e4 1e5],'XLim',[1 1e5])
title('Real data')

% now do the analysis on the DA model
clear ax
ax(4) = subplot(2,3,5); hold on; axis square
plot_tau_gain_nat_stim(syn_data,ax,'response_cutoff',30,'example_history_length',2e3,'method','unscaled_Pearson');
set(ax(4),'YLim',[-15 10],'XTick',[1 10 100 1e3 1e4 1e5],'XLim',[1 1e5])
title('DA model synth data')
act = autoCorrelationTime(z);
plot(ax(4),[act act],[-100 100],'r','LineWidth',3)


prettyFig();

labelFigure

if being_published
	snapnow
	delete(gcf)
end

%%
% What if I use a simple linear model and then fit a NLN model to the data? Do I still see something that could be interpreted as gain control?

R = 10*convolve(time,S,K,filtertime);
R(R<0)=0;
R = 50*sqrt(R);

clear syn_data
syn_data.S =  S;
syn_data.R = R;

clear p
p.k_D = 0.3566;
p.  n = 1.25;

% generate NLN predictions
syn_data.P = NLNmodel([syn_data.S syn_data.R],p);

figure('outerposition',[0 0 1502 505],'PaperUnits','points','PaperSize',[1502 505]); hold on

subplot(1,3,1); hold on
plot(syn_data.P,syn_data.R,'k.')
ylabel('Linear model (Hz)')
xlabel('NLN model prediction (Hz)')
legend(['r^2=' oval(rsquare(syn_data.R,syn_data.P))],'Location','southeast')

% shot the correlations 
subplot(1,3,2); hold on

[a,lags] = autocorr(S,6e4);
plot(lags,a,'k')
legend({'Stimulus'})
set(gca,'XScale','log','XTick',[1 10 100 1e3 1e4 1e5])
xlabel('Lag (ms)')
ylabel('Autocorrelation')

% do the analysis on the real data again
clear ax
ax(4) = subplot(1,3,3); hold on; axis square
plot_tau_gain_nat_stim(data(3),ax,'response_cutoff',40,'example_history_length',2e3,'method','unscaled_Pearson');
set(ax(4),'YLim',[-15 10],'XTick',[1 10 100 1e3 1e4 1e5],'XLim',[1 1e5])


% now do the analysis on the DA model
clear ax
ax(4) = subplot(1,3,3); hold on; axis square
plot_tau_gain_nat_stim(syn_data,ax,'c',[1 0 0],'response_cutoff',30,'example_history_length',2e3,'method','unscaled_Pearson');
set(ax(4),'YLim',[-15 10],'XTick',[1 10 100 1e3 1e4 1e5],'XLim',[1 1e5])
title('{\color{black}Data } {\color{red}Linear model}');

prettyFig();

if being_published
	snapnow
	delete(gcf)
end


%% Version Info
%
pFooter;
