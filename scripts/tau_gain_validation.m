
pHeader;




%% Validation of estimation of gain control timescale
%

   ;;;    ;;;;;;;;   ;;;;;;;     ;;;    
  ;; ;;   ;;     ;; ;;     ;;   ;; ;;   
 ;;   ;;  ;;     ;;        ;;  ;;   ;;  
;;     ;; ;;;;;;;;   ;;;;;;;  ;;     ;; 
;;;;;;;;; ;;     ;; ;;        ;;;;;;;;; 
;;     ;; ;;     ;; ;;        ;;     ;; 
;;     ;; ;;;;;;;;  ;;;;;;;;; ;;     ;; 

 ;;;;;;;          ;;;;;;;;  ;;     ;; ;;;;;;;;    ;;;    ;;    ;;  ;;;;;;;  ;;    ;; ;;;;;;;; 
;;     ;;         ;;     ;; ;;     ;;    ;;      ;; ;;   ;;;   ;; ;;     ;; ;;;   ;; ;;       
       ;;         ;;     ;; ;;     ;;    ;;     ;;   ;;  ;;;;  ;; ;;     ;; ;;;;  ;; ;;       
 ;;;;;;;  ;;;;;;; ;;;;;;;;  ;;     ;;    ;;    ;;     ;; ;; ;; ;; ;;     ;; ;; ;; ;; ;;;;;;   
;;                ;;     ;; ;;     ;;    ;;    ;;;;;;;;; ;;  ;;;; ;;     ;; ;;  ;;;; ;;       
;;                ;;     ;; ;;     ;;    ;;    ;;     ;; ;;   ;;; ;;     ;; ;;   ;;; ;;       
;;;;;;;;;         ;;;;;;;;   ;;;;;;;     ;;    ;;     ;; ;;    ;;  ;;;;;;;  ;;    ;; ;;;;;;;; 


%% ab2A and 2-butanone
% The first dataset I will try this on is with ab2A and 2-butanone. The data we have here is very broadly distributed, as we have the same naturalistic stimulus at three different scales. 



%% 
% Can we see if there is any gain control in this data? In the following figure, I first fit NLN-models neuron-by-neuron to the data. The NLN models have only two parameters (the $k_D$ and the $n$) that are fit parametrically. Other parameters (like the filter) are fit non-parametrically. In the following figure, I plot the results of each neuron in a separate colour. The first plot shows the distribution of the deviations of the NLN model predictions from the measured response. The second plot shows the Spearman correlation between the deviations and the mean stimulus in some preceding window, as a function of window length. Note that these plots tend to have a minimum at some defined timescale. The dotted and dashed lines indicate the autocorrelation times for the stimulus and the response respectively. 


% get all data 
cdata = consolidateData2(getPath(dataManager,'4608c42b12191b383c84fea52392ea97'));
[~, data] =  assembleScaledNatStim(cdata);


clear p

p(1).k_D = .6578;
p(1).n = .7188;

p(2).  k_D = 0.1109;
p(2).    n = .9688;

p(3).k_D = .1187;
p(3).n = .7812;

% generate responses using this model 
warning off
for i = 1:length(data)
	for j = 1:size(data(i).X,2)
		try
			data(i).P(:,j) = NLNmodel([data(i).S(:,j) - min(data(i).S(:,j)) data(i).R(:,j)] ,p(i));
		catch
		end
	end
end
warning on

figure('outerposition',[0 0 1220 601],'PaperUnits','points','PaperSize',[1220 601]); hold on
clear ax
ax(2) = subplot(1,2,1); hold on
ax(4) = subplot(1,2,2); hold on

c = lines(3);

for i = 1:3
	plot_tau_gain_nat_stim(data(i),ax,'c',c(i,:),'response_cutoff',30);
end

prettyFig();
suptitle('ab2A -- 2-butanone')

if being_published
	snapnow
	delete(gcf)
end

%%
% It looks like there is gain control that is not being captured by the NLN model on a timescale of a few seconds. Is this actually true? I need to validate this analysis method to ensure that what we see isn't the result of fitting a model to data that is actually generated by a qualitatively different model. 

%%
% The first order of business is to check if we really need to use Spearman correlations. Can we use simple Pearson correlations? In the following figure, I repeat the same analysis, but using Pearson correlations. 

figure('outerposition',[0 0 1220 601],'PaperUnits','points','PaperSize',[1220 601]); hold on
clear ax
ax(2) = subplot(1,2,1); hold on
ax(4) = subplot(1,2,2); hold on

c = lines(3);

for i = 1:3
	plot_tau_gain_nat_stim(data(i),ax,'c',c(i,:),'method','Pearson');
end

prettyFig();
suptitle('ab2A -- 2-butanone')

if being_published
	snapnow
	delete(gcf)
end


%%
% This doesn't look so different, so that's good. 

%% Validation: response cutoff
% In this analysis, I only analyse the responses to whiffs, and neglect deviations from model fit when there is no stimulus. The reason is because when there is no stimulus, the baseline firing of the neuron deviates randomly from the model responses, which are not very meaningful. However, does this cutoff affect this analysis? To determine this, I generate synthetic responses using a NLN model, and then fit a NLN model back to this, and repeat this analysis with different cutoffs. 

% get the real filter
[~,K] = NLNmodel([data(3).S(:,2) - min(data(3).S(:,2)) data(3).R(:,2)] ,p(3));
filtertime = (1:length(K)) - 51;

clear q
q.n = .7812;
q.k_D = .1187;

% generate synthetic data
clear syn_data
S = data(3).S(:,2);
time = 1:length(S);
a = 1./(1+(q.k_D./S).^q.n);
R = convolve(time,a,K,filtertime);
R(R<0) = 0;
R = R*100;
syn_data.S = S;
syn_data.R = R;
% make predictions
syn_data.P = NLNmodel([S R],q);

figure('outerposition',[0 0 1501 555],'PaperUnits','points','PaperSize',[1501 555]); hold on
clear ax
ax(2) = subplot(1,3,1); hold on; axis square
ax(4) = subplot(1,3,2); hold on; axis square
clear ax2
ax2(4) = subplot(1,3,3); hold on; axis square

all_cutoffs = [0 5 10 20];
c = parula(length(all_cutoffs)+1);
for i = 1:length(all_cutoffs)
	plot_tau_gain_nat_stim(syn_data,ax,'c',c(i,:),'response_cutoff',all_cutoffs(i),'method','Spearman');
	plot_tau_gain_nat_stim(syn_data,ax2,'c',c(i,:),'response_cutoff',all_cutoffs(i),'method','Pearson');
end
suptitle('synthetic data: effect of response cutoff ')


set(ax(4),'YLim',[-1 1])
set(ax2(4),'YLim',[-1 1])

l = flipud(ax(2).Children); clear L
for i = 1:length(l)
	L{i} = ['R>' oval(all_cutoffs(i))];
end
legend(l,L);
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%%
% Varying the response cutoff seems to do all sorts of stuff to these plots. Most importantly, it varies the instantaneous correlation. So perhaps we should set the response cutoff so that the instantaneous correlation is zero? That's one thought. While it is troubling that there seems to be some structure in the correlation vs. timescale plots, note that the deviations from the model are tiny, and all these correlations arise from tiny mismatches. 

%%
% Perhaps a way to combine the effect of the correlation of the deviations and the scale of the deviations would be to measure the slope of the plot of deviations vs. mean stimulus in the preceding window. 


figure('outerposition',[0 0 1501 901],'PaperUnits','points','PaperSize',[1501 901]); hold on

clear ax
ax(2) = subplot(2,4,1); hold on; axis square
ax(3) = subplot(2,4,2); hold on; axis square
ax(4) = subplot(2,4,3); hold on; axis square

clear ax2
ax2(4) = subplot(2,4,4); hold on; axis square

all_cutoffs = [0 5 10 20];
c = parula(length(all_cutoffs)+1);

for i = 1:length(all_cutoffs)
	plot_tau_gain_nat_stim(data(3),ax,'c',c(i,:),'example_history_length',1e3,'method','slope','response_cutoff',all_cutoffs(i));
	plot_tau_gain_nat_stim(data(3),ax2,'c',c(i,:),'example_history_length',1e3,'method','Spearman','response_cutoff',all_cutoffs(i));
end

set(ax(4),'YLim',[-500 200])
set(ax(3),'YLim',[-30 30],'XLim',[1e-4 1])
title(ax(4),'Real data')
title(ax(2),'Real data')
set(ax2(4),'YLim',[-1 1])

% now do the synthetic data

clear ax
ax(2) = subplot(2,4,5); hold on; axis square
ax(3) = subplot(2,4,6); hold on; axis square
ax(4) = subplot(2,4,7); hold on; axis square

clear ax2
ax2(4) = subplot(2,4,8); hold on; axis square

for i = 1:length(all_cutoffs)
	plot_tau_gain_nat_stim(syn_data,ax,'c',c(i,:),'example_history_length',1e3,'response_cutoff',all_cutoffs(i),'method','slope');
	plot_tau_gain_nat_stim(syn_data,ax2,'c',c(i,:),'example_history_length',1e3,'method','Spearman','response_cutoff',all_cutoffs(i));
end

set(ax(4),'YLim',[-500 200])
set(ax2(4),'YLim',[-1 1])
set(ax(2),'XLim',[-40 40])
set(ax(3),'YLim',[-30 30],'XLim',[1e-4 1])
title(ax(4),'synthetic data')
title(ax(2),'synthetic data')

l = flipud(ax(2).Children); clear L
for i = 1:length(l)
	L{i} = ['R>' oval(all_cutoffs(i))];
end
legend(l,L);
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%%
% As suspected, the synthetic data shows much smaller apparent gain changes compared to the real data. So that's something to keep in mind -- not only do the absolute values of the correlation matter, but also the scale of the deviations. 


%% Validation: input nonlinearity 
% In this section, I generate synthetic data using a model where the input nonlinearity does not go to 0 or 1 at its extremities. The NLN model I then fit to this synthetic data cannot capture this input nonlinearity, and is forced to approximate it using a "wrong" functional form. 


all_min_a = [0 .1 .2 .3];
all_max_a = [1 .9 .8 .5];

clear q
q.n = .7812;
q.k_D = .1187;

% generate synthetic data
clear syn_data
for i = 1:length(all_max_a)
	S = data(3).S(:,2);
	a = 1./(1+(q.k_D./S).^q.n);
	a = a*(all_max_a(i) - all_min_a(i)) + all_min_a(i);
	time = 1:length(S);
	R = convolve(time,a,K,filtertime);
	R(R<0) = 0;
	R = R*100;
	syn_data(i).S = S;
	syn_data(i).R = R;
	% make predictions
	syn_data(i).P = NLNmodel([S R],q);
end


c = lines(length(syn_data));

figure('outerposition',[0 0 901 902],'PaperUnits','points','PaperSize',[901 902]); hold on
subplot(2,2,1); hold on
for i = 1:length(all_min_a)
	x = logspace(-4,2,100);
	a = 1./(1+(q.k_D./x).^q.n);
	a = a*(all_max_a(i) - all_min_a(i)) + all_min_a(i);
	plot(x,a,'Color',c(i,:))
end
xlabel('Stimulus (V)')
ylabel('a')
title('Synthetic input nonlinearities')
set(gca,'XScale','log')
axis square

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square


for i = 1:length(syn_data)
	rc = findResponseCutoffTauGainPlot(syn_data(i).S,syn_data(i).R,syn_data(i).P,'true');
	plot_tau_gain_nat_stim(syn_data(i),ax,'c',c(i,:),'response_cutoff',rc,'example_history_length',1e3);
end
suptitle('synthetic data: effect of input nonlinearity mismatch')
set(ax(4),'YLim',[-1 1])
set(ax(3),'YLim',[-30 30])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%%
% Now I choose a qualitatively different input nonlinearity -- a log function, and repeat the analysis to see if the timescale we saw in the data pops back up. 

a = log(S);
a = a - min(a); a = a/max(a);
R = convolve(time,a,K,filtertime);
R(R<0) = 0;
R = R*100;

clear syn_data
syn_data.S = S;
syn_data.R = R;

% find best fit model params
clear p
p.n = 0.53;
p.k_D = .0313;

% make predictions
syn_data.P = NLNmodel([S R],p);


figure('outerposition',[0 0 902 901],'PaperUnits','points','PaperSize',[902 901]); hold on
subplot(2,2,1); hold on
x = logspace(-4,2,100);
a = log(x);
a = a - min(a); a = a/max(a);
plot(x,a,'k');
a = 1./(1 + (p.k_D./x).^p.n);
plot(x,a,'r')

xlabel('Stimulus (V)')
ylabel('a')
title('Synthetic input nonlinearities: a log')
set(gca,'XScale','log')
axis square

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square

plot_tau_gain_nat_stim(syn_data,ax,'c',[0 0 0],'response_cutoff',10,'method','Spearman');

suptitle('synthetic data')
set(ax(4),'YLim',[-1 1])
set(ax(3),'YLim',[-30 30])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%% Validation: effect of stimulus correlation time
% Do correlations in th stimulus manifest in weird ways and cause us to think that there is gain control when there isn't? To check this, I rescale the stimulus in the time axis and re-do the analysis to see how stimulus correlations can screw with this method. 

% generate data

clear p
p.k_D = .1187;
p.n = .7812;

stim_scale = [.2 .5 1 2 5];

clear syn_data
for i = 1:length(stim_scale)
	fake_time = linspace(0,max(time),stim_scale(i)*length(time));
	Si  = vectorise(interp1(time,S,fake_time));

	% generate responses using real model
	a = 1./(1 + (p.k_D./Si).^p.n);
	R = convolve(1:length(Si),a,K,filtertime);
	R(R<0) = 0;
	R = R*100;

	% remove nans
	rm_this = isnan(R) | isnan(Si);

	syn_data(i).R = R(~rm_this);
	syn_data(i).S = Si(~rm_this);

	% make prediction
	syn_data(i).P = NLNmodel([ Si(~rm_this) R(~rm_this)],p);
end

c = lines(length(syn_data));

figure('outerposition',[0 0 903 901],'PaperUnits','points','PaperSize',[903 901]); hold on
subplot(2,2,1); hold on
for i = 1:length(syn_data)
	[a,lags] = autocorr(syn_data(i).S,length(syn_data(i).S)-1);
	plot(lags,a,'Color',c(i,:))
end
set(gca,'XScale','log')
xlabel('lag (ms)')
ylabel('Autocorrelation')

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square


for i = 1:length(syn_data)
	rc = findResponseCutoffTauGainPlot(syn_data(i).S,syn_data(i).R,syn_data(i).P,true);
	plot_tau_gain_nat_stim(syn_data(i),ax,'c',c(i,:),'response_cutoff',rc);
end
suptitle('synthetic data: effect of changing stimulus correlations')
set(ax(4),'YLim',[-1 1])
set(ax(3),'YLim',[-30 30])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end

%% Validation: adapting NLN model
% In this section, I use an adapting NLN model and attempt to recover the timescale of gain control using this analysis. 

clear p
p.   k0 = 0.0451;
p.tau_z = 2424;
p.    B = 0.6250;
p.  n_z = 0.1016;
p.    n = 0.9812;
p. tau1 = 41.2812;
p. tau2 = 45.7812;
p.  n_y = 1.3984;
p.    A = 0.9340;
p.    C = 2.7945e+03;

[R,~,~,k_D] = aNLN2(S,p);

clear syn_data
syn_data.R = R;
syn_data.S = S;

clear p
p.k_D = 0.7589;
p.n = 0.9000;

syn_data.P = NLNmodel([S R],p);

figure('outerposition',[0 0 903 901],'PaperUnits','points','PaperSize',[903 901]); hold on
subplot(2,2,1); hold on

[a,lags] = autocorr(k_D,length(k_D)-1);
plot(lags,a,'Color','k')

set(gca,'XScale','log','XTick',[1 10 100 1e3 1e4 1e5])
xlabel('lag (ms)')
ylabel('Autocorrelation')
title('Autocorrelation of k_D')

clear ax
ax(2) = subplot(2,2,2); hold on; axis square
ax(3) = subplot(2,2,3); hold on; axis square
ax(4) = subplot(2,2,4); hold on; axis square

plot_tau_gain_nat_stim(syn_data,ax,'c',[0 0 0],'response_cutoff',10,'example_history_length',2e3);

suptitle('synthetic data: adapting NLN model fit with NLN model')
set(ax(4),'YLim',[-1 1])
set(ax(3),'YLim',[-50 60],'XLim',[1e-5 1e0],'XTick',[1e-5 1e-4 1e-3 1e-2 1e-1 1e0])
prettyFig();

if being_published
	snapnow
	delete(gcf)
end


%% Version Info
%
pFooter;
